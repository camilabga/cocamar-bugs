{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8boDWfTt6a97",
        "DcfAFunl6jY0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camilabga/cocamar-bugs/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tRBeUodgrrq"
      },
      "source": [
        "# 1.0 Useful classes and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B47RhEPVhIxP"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import h5py\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlHvOPCsZ5yC"
      },
      "source": [
        "## 1.1 HDF5DatasetWriter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-IfBT8dhDiD"
      },
      "source": [
        "# import the necessary packages\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "class HDF5DatasetWriter:\n",
        "  def __init__(self, dims, outputPath, dataKey=\"images\",bufSize=1000):\n",
        "    \"\"\"\n",
        "    The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n",
        "    \n",
        "    Args:\n",
        "    dims: controls the dimension or shape of the data we will be storing in the dataset.\n",
        "    if we were storing the (flattened) raw pixel intensities of the 28x28 = 784 MNIST dataset, \n",
        "    then dims=(70000, 784).\n",
        "    outputPath: path to where our output HDF5 file will be stored on disk.\n",
        "    datakey: The optional dataKey is the name of the dataset that will store\n",
        "    the data our algorithm will learn from.\n",
        "    bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature\n",
        "    vectors/images. Once we reach bufSize, we’ll flush the buffer to the HDF5 dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # check to see if the output path exists, and if so, raise\n",
        "    # an exception\n",
        "    if os.path.exists(outputPath):\n",
        "      raise ValueError(\"The supplied `outputPath` already \"\n",
        "        \"exists and cannot be overwritten. Manually delete \"\n",
        "        \"the file before continuing.\", outputPath)\n",
        "\n",
        "    # open the HDF5 database for writing and create two datasets:\n",
        "    # one to store the images/features and another to store the\n",
        "    # class labels\n",
        "    self.db = h5py.File(outputPath, \"w\")\n",
        "    self.data = self.db.create_dataset(dataKey, dims,dtype=\"float\",compression='gzip')\n",
        "    self.labels = self.db.create_dataset(\"labels\", (dims[0],),dtype=\"int\")\n",
        "\n",
        "    # store the buffer size, then initialize the buffer itself\n",
        "    # along with the index into the datasets\n",
        "    self.bufSize = bufSize\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add(self, rows, labels):\n",
        "    # add the rows and labels to the buffer\n",
        "    self.buffer[\"data\"].extend(rows)\n",
        "    self.buffer[\"labels\"].extend(labels)\n",
        "\n",
        "    # check to see if the buffer needs to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) >= self.bufSize:\n",
        "      self.flush()\n",
        "\n",
        "  def flush(self):\n",
        "    # write the buffers to disk then reset the buffer\n",
        "    i = self.idx + len(self.buffer[\"data\"])\n",
        "    self.data[self.idx:i] = self.buffer[\"data\"]\n",
        "    self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
        "    self.idx = i\n",
        "    self.buffer = {\"data\": [], \"labels\": []}\n",
        "\n",
        "  def storeClassLabels(self, classLabels):\n",
        "    # create a dataset to store the actual class label names,\n",
        "    # then store the class labels\n",
        "    dt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n",
        "    labelSet = self.db.create_dataset(\"label_names\",(len(classLabels),), dtype=dt)\n",
        "    labelSet[:] = classLabels\n",
        "\n",
        "  def close(self):\n",
        "    # check to see if there are any other entries in the buffer\n",
        "    # that need to be flushed to disk\n",
        "    if len(self.buffer[\"data\"]) > 0:\n",
        "      self.flush()\n",
        "\n",
        "    # close the dataset\n",
        "    self.db.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGZswDVOaC3C"
      },
      "source": [
        "## 1.2 Image to Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgmBX1ST1F8U"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcDGt0BnaGXY"
      },
      "source": [
        "## 1.3 AspectAware"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ga-FgPH1NuW"
      },
      "source": [
        "# import the necessary packages\n",
        "import imutils\n",
        "import cv2\n",
        "#fdafjkldshfj ldhsjaksldhfajkdlhfa jsdhfli hdasuklfhsdkfj la\n",
        "###dsf asdfhkjahfksj\n",
        "\n",
        "# useful class to help the resize of images\n",
        "class AspectAwarePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# grab the dimensions of the image and then initialize\n",
        "\t\t# the deltas to use when cropping\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tdW = 0\n",
        "\t\tdH = 0\n",
        "\n",
        "\t\t# if the width is smaller than the height, then resize\n",
        "\t\t# along the width (i.e., the smaller dimension) and then\n",
        "\t\t# update the deltas to crop the height to the desired\n",
        "\t\t# dimension\n",
        "\t\tif w < h:\n",
        "\t\t\timage = imutils.resize(image, width=self.width,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
        "\n",
        "\t\t# otherwise, the height is smaller than the width so\n",
        "\t\t# resize along the height and then update the deltas\n",
        "\t\t# crop along the width\n",
        "\t\telse:\n",
        "\t\t\timage = imutils.resize(image, height=self.height,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
        "\n",
        "\t\t# now that our images have been resized, we need to\n",
        "\t\t# re-grab the width and height, followed by performing\n",
        "\t\t# the crop\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\timage = image[dH:h - dH, dW:w - dW]\n",
        "\n",
        "\t\t# finally, resize the image to the provided spatial\n",
        "\t\t# dimensions to ensure our output image is always a fixed\n",
        "\t\t# size\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XgdXcA7aPfT"
      },
      "source": [
        "## 1.4 Mean preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgq3q5uLZj-l"
      },
      "source": [
        "Let’s get started with the mean pre-processor. We will learn how to convert an image\n",
        "dataset to HDF5 format – part of this conversion involved computing the average Red, Green, and Blue pixel intensities across all images in the training dataset. Now that we have these averages, we are going to perform a pixel-wise subtraction of these values from our input images as a **form of data normalization**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfbg-dCsUEuW"
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "\n",
        "class MeanPreprocessor:\n",
        "\tdef __init__(self, rMean, gMean, bMean):\n",
        "\t\t# store the Red, Green, and Blue channel averages across a\n",
        "\t\t# training set\n",
        "\t\tself.rMean = rMean\n",
        "\t\tself.gMean = gMean\n",
        "\t\tself.bMean = bMean\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# split the image into its respective Red, Green, and Blue\n",
        "\t\t# channels\n",
        "\t\t(B, G, R) = cv2.split(image.astype(\"float32\"))\n",
        "\n",
        "\t\t# subtract the means for each channel\n",
        "\t\tR -= self.rMean\n",
        "\t\tG -= self.gMean\n",
        "\t\tB -= self.bMean\n",
        "\n",
        "    # Keep in mind that OpenCV represents images in BGR order\n",
        "\t\t# merge the channels back together and return the image\n",
        "\t\treturn cv2.merge([B, G, R])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COixYybsaWyp"
      },
      "source": [
        "## 1.5 Patch preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxZ6UF7ahNo"
      },
      "source": [
        "The PatchPreprocessor is responsible for randomly sampling MxN regions of an image during the training process. We apply patch preprocessing when the spatial dimensions of our input images are larger than what the CNN expects – this is a common technique to help reduce overfitting, and is, therefore, **a form of regularization**. Instead of using the entire image during training, we instead crop a random portion of it and pass it to the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPeYByPoa6C4"
      },
      "source": [
        "As you will see, we will construct an HDF5 dataset of Kaggle Dogs vs. Cats images where each image is 256x256 pixels. However, the AlexNet architecture that we’ll be implementing later in this lesson can only accept images of size 227x227 pixels. This is an excellent opportunity to perform data augmentation by randomly cropping a 227x227 region from the 256x256 image during training using PatchPreprocessor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm2bRAs2UKHj"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "\n",
        "class PatchPreprocessor:\n",
        "\tdef __init__(self, width, height):\n",
        "\t\t# store the target width and height of the image\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# extract a random crop from the image with the target width\n",
        "\t\t# and height\n",
        "\t\treturn extract_patches_2d(image, (self.height, self.width),\n",
        "\t\t\tmax_patches=1)[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gft-PE3aJvZ"
      },
      "source": [
        "## 1.6 Crop preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFW6NqV2eSYY"
      },
      "source": [
        "Next, we need to define a CropPreprocessor responsible for computing the 10-crops for oversampling. During the evaluating phase of our CNN, we’ll crop the four corners of the input image + the center region and then take their corresponding horizontal flips, for a total of ten samples per input image.\n",
        "\n",
        "These ten samples will be passed through the CNN, and then the probabilities averaged.\n",
        "Applying this over-sampling method tends to include 1-2 percent increases in classification accuracy (and in some cases, even higher)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc0kftNdR2lk"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class CropPreprocessor:\n",
        "\tdef __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, whether or not\n",
        "\t\t# horizontal flips should be included, along with the\n",
        "\t\t# interpolation method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.horiz = horiz\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# initialize the list of crops\n",
        "\t\tcrops = []\n",
        "\n",
        "\t\t# grab the width and height of the image then use these\n",
        "\t\t# dimensions to define the corners of the image based\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tcoords = [\n",
        "\t\t\t[0, 0, self.width, self.height],\n",
        "\t\t\t[w - self.width, 0, w, self.height],\n",
        "\t\t\t[w - self.width, h - self.height, w, h],\n",
        "\t\t\t[0, h - self.height, self.width, h]]\n",
        "\n",
        "\t\t# compute the center crop of the image as well\n",
        "\t\tdW = int(0.5 * (w - self.width))\n",
        "\t\tdH = int(0.5 * (h - self.height))\n",
        "\t\tcoords.append([dW, dH, w - dW, h - dH])\n",
        "\n",
        "\t\t# loop over the coordinates, extract each of the crops,\n",
        "\t\t# and resize each of them to a fixed size\n",
        "\t\tfor (startX, startY, endX, endY) in coords:\n",
        "\t\t\tcrop = image[startY:endY, startX:endX]\n",
        "\t\t\tcrop = cv2.resize(crop, (self.width, self.height),\n",
        "\t\t\t\tinterpolation=self.inter)\n",
        "\t\t\tcrops.append(crop)\n",
        "\n",
        "\t\t# check to see if the horizontal flips should be taken\n",
        "\t\tif self.horiz:\n",
        "\t\t\t# compute the horizontal mirror flips for each crop\n",
        "\t\t\tmirrors = [cv2.flip(c, 1) for c in crops]\n",
        "\t\t\tcrops.extend(mirrors)\n",
        "\n",
        "\t\t# return the set of crops\n",
        "\t\treturn np.array(crops)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NF8faSog8so"
      },
      "source": [
        "## 1.7 HDF5 dataset generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soLUcyH1iCOO"
      },
      "source": [
        "Before we can implement the AlexNet architecture and train it on the Kaggle Dogs vs. Cats dataset, we first need to define a class responsible for yielding batches of images and labels from our HDF5 dataset. Section 1.1 discussed how to convert a set of images residing on disk into an HDF5 dataset – but how do we get them back out again? The answer is to define an **HDF5DatasetGenerator** class.\n",
        "\n",
        "Previously, all of our image datasets could be loaded into memory so we could rely on Keras generator utilities to yield our batches of images and corresponding labels. However, now that our datasets are too large to fit into memory, we need to handle implementing this generator ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMo22QKUg7me"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "class HDF5DatasetGenerator:\n",
        "\tdef __init__(self, dbPath, batchSize, preprocessors=None, aug=None, binarize=True, classes=2):\n",
        "\t\t# store the batch size, preprocessors, and data augmentor,\n",
        "\t\t# whether or not the labels should be binarized, along with\n",
        "\t\t# the total number of classes\n",
        "\t\tself.batchSize = batchSize\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\t\tself.aug = aug\n",
        "\t\tself.binarize = binarize\n",
        "\t\tself.classes = classes\n",
        "\n",
        "\t\t# open the HDF5 database for reading and determine the total\n",
        "\t\t# number of entries in the database\n",
        "\t\tself.db = h5py.File(dbPath, \"r\")\n",
        "\t\tself.numImages = self.db[\"labels\"].shape[0]\n",
        "\n",
        "\tdef generator(self, passes=np.inf):\n",
        "\t\t# initialize the epoch count\n",
        "\t\tepochs = 0\n",
        "\n",
        "\t\t# keep looping infinitely -- the model will stop once we have\n",
        "\t\t# reach the desired number of epochs\n",
        "\t\twhile epochs < passes:\n",
        "\t\t\t# loop over the HDF5 dataset\n",
        "\t\t\tfor i in np.arange(0, self.numImages, self.batchSize):\n",
        "\t\t\t\t# extract the images and labels from the HDF dataset\n",
        "\t\t\t\timages = self.db[\"images\"][i: i + self.batchSize]\n",
        "\t\t\t\tlabels = self.db[\"labels\"][i: i + self.batchSize]\n",
        "\n",
        "\t\t\t\t# check to see if the labels should be binarized\n",
        "\t\t\t\tif self.binarize:\n",
        "\t\t\t\t\tlabels = to_categorical(labels,\n",
        "\t\t\t\t\t\tself.classes)\n",
        "\n",
        "\t\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t\t# initialize the list of processed images\n",
        "\t\t\t\t\tprocImages = []\n",
        "\n",
        "\t\t\t\t\t# loop over the images\n",
        "\t\t\t\t\tfor image in images:\n",
        "\t\t\t\t\t\t# loop over the preprocessors and apply each\n",
        "\t\t\t\t\t\t# to the image\n",
        "\t\t\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t\t\t\t# update the list of processed images\n",
        "\t\t\t\t\t\tprocImages.append(image)\n",
        "\n",
        "\t\t\t\t\t# update the images array to be the processed\n",
        "\t\t\t\t\t# images\n",
        "\t\t\t\t\timages = np.array(procImages)\n",
        "\n",
        "\t\t\t\t# if the data augmenator exists, apply it\n",
        "\t\t\t\tif self.aug is not None:\n",
        "\t\t\t\t\t(images, labels) = next(self.aug.flow(images,\n",
        "\t\t\t\t\t\tlabels, batch_size=self.batchSize))\n",
        "\n",
        "\t\t\t\t# yield a tuple of images and labels\n",
        "\t\t\t\tyield (images, labels)\n",
        "\n",
        "\t\t\t# increment the total number of epochs\n",
        "\t\t\tepochs += 1\n",
        "\n",
        "\tdef close(self):\n",
        "\t\t# close the database\n",
        "\t\tself.db.close()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ou-_XnZqjrY"
      },
      "source": [
        "## 1.8 Simple preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juf1YRHzqmeU"
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "\n",
        "class SimplePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# resize the image to a fixed size, ignoring the aspect\n",
        "\t\t# ratio\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bJUx9Prq2xE"
      },
      "source": [
        "## 1.9 Training monitor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1JGywYeq5Wj"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath):\n",
        "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\n",
        "\n",
        "\t\t\t\t# check to see if a starting epoch was supplied\n",
        "\t\t\t\tif self.startAt > 0:\n",
        "\t\t\t\t\t# loop over the entries in the history log and\n",
        "\t\t\t\t\t# trim any entries that are past the starting\n",
        "\t\t\t\t\t# epoch\n",
        "\t\t\t\t\tfor k in self.H.keys():\n",
        "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\n",
        "\t\t# ensure at least two epochs have passed before plotting\n",
        "\t\t# (epoch starts at zero)\n",
        "\t\tif len(self.H[\"loss\"]) > 1:\n",
        "\t\t\t# plot the training loss and accuracy\n",
        "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\n",
        "\t\t\tplt.style.use(\"ggplot\")\n",
        "\t\t\tplt.figure()\n",
        "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n",
        "\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\n",
        "\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\n",
        "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n",
        "\t\t\t\tlen(self.H[\"loss\"])))\n",
        "\t\t\tplt.xlabel(\"Epoch #\")\n",
        "\t\t\tplt.ylabel(\"Loss/Accuracy\")\n",
        "\t\t\tplt.legend()\n",
        "\n",
        "\t\t\t# save the figure\n",
        "\t\t\tplt.savefig(self.figPath)\n",
        "\t\t\tplt.close()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk2HBr6JAMQ8"
      },
      "source": [
        "## 1.10 Rank accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJCbMd1sAPP8"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "\n",
        "def rank5_accuracy(preds, labels):\n",
        "\t# initialize the rank-1 and rank-5 accuracies\n",
        "\trank1 = 0\n",
        "\trank5 = 0\n",
        "\n",
        "\t# loop over the predictions and ground-truth labels\n",
        "\tfor (p, gt) in zip(preds, labels):\n",
        "\t\t# sort the probabilities by their index in descending\n",
        "\t\t# order so that the more confident guesses are at the\n",
        "\t\t# front of the list\n",
        "\t\tp = np.argsort(p)[::-1]\n",
        "\n",
        "\t\t# check if the ground-truth label is in the top-5\n",
        "\t\t# predictions\n",
        "\t\tif gt in p[:5]:\n",
        "\t\t\trank5 += 1\n",
        "\n",
        "\t\t# check to see if the ground-truth is the #1 prediction\n",
        "\t\tif gt == p[0]:\n",
        "\t\t\trank1 += 1\n",
        "\n",
        "\t# compute the final rank-1 and rank-5 accuracies\n",
        "\trank1 /= float(len(preds))\n",
        "\trank5 /= float(len(preds))\n",
        "\n",
        "\t# return a tuple of the rank-1 and rank-5 accuracies\n",
        "\treturn (rank1, rank5)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrmCQKbMHI-4"
      },
      "source": [
        "## 1.11 SimpleDatasetLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj4RbTs8HH62"
      },
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# helper to load images\n",
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data), np.array(labels))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwDVpznk2rZ0"
      },
      "source": [
        "# 2.0 Working with HDFS and large datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "K9e4A2OIfmDO",
        "outputId": "1f256a8d-8ed6-45b9-ed1b-658c064f74ed"
      },
      "source": [
        "!unzip \"data.zip\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/train/\n",
            "  inflating: data/train/lagarta.5.jpg  \n",
            "  inflating: data/train/lagarta.6.jpg  \n",
            "  inflating: data/train/lagarta.7.jpg  \n",
            "  inflating: data/train/lagarta.8.jpg  \n",
            "  inflating: data/train/lagarta.9.jpg  \n",
            "  inflating: data/train/lagarta.10.jpg  \n",
            "  inflating: data/train/lagarta.13.jpg  \n",
            "  inflating: data/train/lagarta.14.jpg  \n",
            "  inflating: data/train/lagarta.1.jpg  \n",
            "  inflating: data/train/lagarta.2.jpg  \n",
            "  inflating: data/train/lagarta.11.jpg  \n",
            "  inflating: data/train/lagarta.12.jpg  \n",
            "  inflating: data/train/lagarta.15.jpg  \n",
            "  inflating: data/train/lagarta.3.png  \n",
            "  inflating: data/train/lagarta.4.jpg  \n",
            "  inflating: data/train/lagarta.17.jpg  \n",
            "  inflating: data/train/lagarta.16.jpg  \n",
            "  inflating: data/train/lagarta.18.jfif  \n",
            "  inflating: data/train/lagarta.19.jfif  \n",
            "  inflating: data/train/lagarta.20.jpg  \n",
            "  inflating: data/train/lagarta.21.jpg  \n",
            "  inflating: data/train/lagarta.22.png  \n",
            "  inflating: data/train/lagarta.23.png  \n",
            "  inflating: data/train/lagarta.27.png  \n",
            "  inflating: data/train/lagarta.26.JPG  \n",
            "  inflating: data/train/lagarta.25.jpeg  \n",
            "  inflating: data/train/lagarta.28.jpg  \n",
            "  inflating: data/train/lagarta.29.jfif  \n",
            "  inflating: data/train/lagarta.24.jfif  \n",
            "  inflating: data/train/negative.1.jpg  \n",
            "  inflating: data/train/negative.2.jpg  \n",
            "  inflating: data/train/negative.3.jpg  \n",
            "  inflating: data/train/negative.4.jpg  \n",
            "  inflating: data/train/negative.5.jpg  \n",
            "  inflating: data/train/negative.6.jpg  \n",
            "  inflating: data/train/negative.7.jpg  \n",
            "  inflating: data/train/negative.8.jpg  \n",
            "  inflating: data/train/negative.9.jpg  \n",
            "  inflating: data/train/negative.10.jpg  \n",
            "  inflating: data/train/negative.11.jpg  \n",
            "  inflating: data/train/negative.12.jpg  \n",
            "  inflating: data/train/negative.13.jpg  \n",
            "  inflating: data/train/negative.14.jpg  \n",
            "  inflating: data/train/negative.15.jpg  \n",
            "  inflating: data/train/negative.16.jpg  \n",
            "  inflating: data/train/negative.17.jpg  \n",
            "  inflating: data/train/negative.18.jpg  \n",
            "  inflating: data/train/negative.19.jpg  \n",
            "  inflating: data/train/negative.20.jpg  \n",
            "  inflating: data/train/negative.21.jpg  \n",
            "  inflating: data/train/negative.22.jpg  \n",
            "  inflating: data/train/negative.23.jpg  \n",
            "  inflating: data/train/negative.24.jpg  \n",
            "  inflating: data/train/negative.25.jpg  \n",
            "  inflating: data/train/negative.26.jpg  \n",
            "  inflating: data/train/negative.27.jpg  \n",
            "  inflating: data/train/negative.28.jpg  \n",
            "  inflating: data/train/negative.29.jpg  \n",
            "  inflating: data/train/negative.30.jpg  \n",
            "  inflating: data/train/negative.31.jpg  \n",
            "  inflating: data/train/negative.32.jpg  \n",
            "  inflating: data/train/negative.34.jpg  \n",
            "  inflating: data/train/negative.35.jpg  \n",
            "  inflating: data/train/negative.36.jpg  \n",
            "  inflating: data/train/negative.37.png  \n",
            "  inflating: data/train/negative.38.jpg  \n",
            "  inflating: data/train/negative.39.jpg  \n",
            "  inflating: data/train/negative.40.jpg  \n",
            "  inflating: data/train/negative.41.jpg  \n",
            "  inflating: data/train/negative.42.jpg  \n",
            "  inflating: data/train/negative.43.jpg  \n",
            "  inflating: data/train/negative.44.jpg  \n",
            "  inflating: data/train/negative.45.jpg  \n",
            "  inflating: data/train/negative.46.jpg  \n",
            "  inflating: data/train/negative.47.jpg  \n",
            "  inflating: data/train/negative.48.jpg  \n",
            "  inflating: data/train/negative.49.jpg  \n",
            "  inflating: data/train/negative.50.jpg  \n",
            "  inflating: data/train/negative.51.jpg  \n",
            "  inflating: data/train/negative.52.jpg  \n",
            "  inflating: data/train/negative.53.jpg  \n",
            "  inflating: data/train/negative.54.jpg  \n",
            "  inflating: data/train/negative.55.jpg  \n",
            "  inflating: data/train/negative.56.jpg  \n",
            "  inflating: data/train/negative.57.jpg  \n",
            "  inflating: data/train/negative.58.jpg  \n",
            "  inflating: data/train/negative.59.jpg  \n",
            "  inflating: data/train/negative.60.jpg  \n",
            "  inflating: data/train/negative.61.jpg  \n",
            "  inflating: data/train/negative.62.png  \n",
            "  inflating: data/train/negative.63.png  \n",
            "  inflating: data/train/negative.64.jpg  \n",
            "  inflating: data/train/negative.65.jpg  \n",
            "  inflating: data/train/negative.74.jpg  \n",
            "  inflating: data/train/negative.75.jpg  \n",
            "  inflating: data/train/negative.76.jpg  \n",
            "  inflating: data/train/negative.77.jpg  \n",
            "  inflating: data/train/negative.78.jpg  \n",
            "  inflating: data/train/negative.79.jpg  \n",
            "  inflating: data/train/negative.80.jpg  \n",
            "  inflating: data/train/negative.81.jpg  \n",
            "  inflating: data/train/negative.82.jpg  \n",
            "  inflating: data/train/negative.66.jpg  \n",
            "  inflating: data/train/negative.83.jpg  \n",
            "  inflating: data/train/negative.84.jpg  \n",
            "  inflating: data/train/negative.85.jpg  \n",
            "  inflating: data/train/negative.86.jpg  \n",
            "  inflating: data/train/negative.87.jpg  \n",
            "  inflating: data/train/negative.88.jpg  \n",
            "  inflating: data/train/negative.89.jpg  \n",
            "  inflating: data/train/negative.90.jpg  \n",
            "  inflating: data/train/negative.91.jpg  \n",
            "  inflating: data/train/negative.92.jpg  \n",
            "  inflating: data/train/negative.67.jpg  \n",
            "  inflating: data/train/negative.93.jpg  \n",
            "  inflating: data/train/negative.94.jpg  \n",
            "  inflating: data/train/negative.95.jpg  \n",
            "  inflating: data/train/negative.96.jpg  \n",
            "  inflating: data/train/negative.97.jpg  \n",
            "  inflating: data/train/negative.98.jpg  \n",
            "  inflating: data/train/negative.99.jpg  \n",
            "  inflating: data/train/negative.100.jpg  \n",
            "  inflating: data/train/negative.101.jpg  \n",
            "  inflating: data/train/negative.68.jpg  \n",
            "  inflating: data/train/negative.102.jpg  \n",
            "  inflating: data/train/negative.103.jpg  \n",
            "  inflating: data/train/negative.104.jpg  \n",
            "  inflating: data/train/negative.105.jpg  \n",
            "  inflating: data/train/negative.106.jpg  \n",
            "  inflating: data/train/negative.107.jpg  \n",
            "  inflating: data/train/negative.108.png  \n",
            "  inflating: data/train/negative.109.jpg  \n",
            "  inflating: data/train/negative.69.jpg  \n",
            "  inflating: data/train/negative.70.jpg  \n",
            "  inflating: data/train/negative.71.jpg  \n",
            "  inflating: data/train/negative.72.jpg  \n",
            "  inflating: data/train/negative.73.jpg  \n",
            "  inflating: data/train/negative.110.jpg  \n",
            "  inflating: data/train/negative.111.jpg  \n",
            "  inflating: data/train/negative.112.jpg  \n",
            "  inflating: data/train/negative.113.jpg  \n",
            "  inflating: data/train/negative.114.jpeg  \n",
            "  inflating: data/train/negative.115.jpg  \n",
            "  inflating: data/train/negative.116.jpg  \n",
            "  inflating: data/train/negative.117.jpg  \n",
            "  inflating: data/train/negative.118.jpg  \n",
            "  inflating: data/train/negative.119.jpg  \n",
            "  inflating: data/train/negative.120.jpg  \n",
            "  inflating: data/train/negative.121.jpg  \n",
            "  inflating: data/train/negative.122.jpg  \n",
            "  inflating: data/train/negative.123.jpg  \n",
            "  inflating: data/train/negative.124.jpg  \n",
            "  inflating: data/train/negative.125.jpg  \n",
            "  inflating: data/train/negative.33.png  \n",
            "  inflating: data/train/percevejo_marrom.3.jpg  \n",
            "  inflating: data/train/percevejo_marrom.15.jpg  \n",
            "  inflating: data/train/percevejo_marrom.6.jpg  \n",
            "  inflating: data/train/percevejo_marrom.7.jpg  \n",
            "  inflating: data/train/percevejo_marrom.8.jpg  \n",
            "  inflating: data/train/percevejo_marrom.9.jpg  \n",
            "  inflating: data/train/percevejo_marrom.10.jpg  \n",
            "  inflating: data/train/percevejo_marrom.22.jpg  \n",
            "  inflating: data/train/percevejo_marrom.23.jpg  \n",
            "  inflating: data/train/percevejo_marrom.24.jpg  \n",
            "  inflating: data/train/percevejo_marrom.16.jpg  \n",
            "  inflating: data/train/percevejo_marrom.25.jpg  \n",
            "  inflating: data/train/percevejo_marrom.17.jpg  \n",
            "  inflating: data/train/percevejo_marrom.11.jpg  \n",
            "  inflating: data/train/percevejo_marrom.12.jpg  \n",
            "  inflating: data/train/percevejo_marrom.18.jpg  \n",
            "  inflating: data/train/percevejo_marrom.4.jpg  \n",
            "  inflating: data/train/percevejo_marrom.26.jpg  \n",
            "  inflating: data/train/percevejo_marrom.19.jpg  \n",
            "  inflating: data/train/percevejo_marrom.1.jpg  \n",
            "  inflating: data/train/percevejo_marrom.20.jpg  \n",
            "  inflating: data/train/percevejo_marrom.13.jpg  \n",
            "  inflating: data/train/percevejo_marrom.14.jpg  \n",
            "  inflating: data/train/percevejo_marrom.2.jpg  \n",
            "  inflating: data/train/percevejo_marrom.21.jpg  \n",
            "  inflating: data/train/percevejo_marrom.5.jpg  \n",
            "  inflating: data/train/percevejo_marrom.27.jpg  \n",
            "  inflating: data/train/percevejo_marrom.28.jpg  \n",
            "  inflating: data/train/percevejo_marrom.29.jpg  \n",
            "  inflating: data/train/percevejo_marrom.30.jpg  \n",
            "  inflating: data/train/percevejo_marrom.31.jpg  \n",
            "  inflating: data/train/percevejo_marrom.33.jpg  \n",
            "  inflating: data/train/percevejo_marrom.32.jpg  \n",
            "  inflating: data/train/percevejo_marrom.34.jpg  \n",
            "  inflating: data/train/percevejo_marrom.35.png  \n",
            "  inflating: data/train/percevejo_marrom.36.JPG  \n",
            "  inflating: data/train/percevejo_marrom.39.jpg  \n",
            "  inflating: data/train/percevejo_marrom.37.jpg  \n",
            "  inflating: data/train/percevejo_marrom.38.jpg  \n",
            "  inflating: data/train/percevejo_marrom.40.jpg  \n",
            "  inflating: data/train/percevejo_marrom.41.png  \n",
            "  inflating: data/train/percevejo_marrom.42.jpg  \n",
            "  inflating: data/train/percevejo_marrom.43.jfif  \n",
            "  inflating: data/train/percevejo_marrom.44.jpg  \n",
            "  inflating: data/train/percevejo_marrom.45.jpg  \n",
            "  inflating: data/train/percevejo_marrom.47.png  \n",
            "  inflating: data/train/percevejo_marrom.46.png  \n",
            "  inflating: data/train/percevejo_marrom.48.jpg  \n",
            "  inflating: data/train/percevejo_marrom.49.jpg  \n",
            "  inflating: data/train/percevejo_marrom.50.jpg  \n",
            "  inflating: data/train/percevejo_marrom.51.jpg  \n",
            "  inflating: data/train/percevejo_marrom.52.jpg  \n",
            "  inflating: data/train/percevejo_marrom.53.jpg  \n",
            "  inflating: data/train/percevejo_marrom.54.jpg  \n",
            "  inflating: data/train/percevejo_marrom.55.jpg  \n",
            "  inflating: data/train/percevejo_marrom.56.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.1.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.2.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.3.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.4.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.5.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.6.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.7.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.8.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.9.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.21.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.10.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.11.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.12.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.13.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.14.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.15.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.16.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.17.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.18.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.19.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.20.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.22.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.23.jpg  \n",
            "  inflating: data/train/percevejo_pequeno.24.jpg  \n",
            "  inflating: data/train/percevejo_verde.1.jpg  \n",
            "  inflating: data/train/percevejo_verde.2.jpg  \n",
            "  inflating: data/train/percevejo_verde.3.jpg  \n",
            "  inflating: data/train/percevejo_verde.4.jpg  \n",
            "  inflating: data/train/percevejo_verde.5.jpg  \n",
            "  inflating: data/train/percevejo_verde.20.jpg  \n",
            "  inflating: data/train/percevejo_verde.6.jpg  \n",
            "  inflating: data/train/percevejo_verde.30.png  \n",
            "  inflating: data/train/percevejo_verde.7.jpg  \n",
            "  inflating: data/train/percevejo_verde.21.jpg  \n",
            "  inflating: data/train/percevejo_verde.22.jpg  \n",
            "  inflating: data/train/percevejo_verde.23.jpg  \n",
            "  inflating: data/train/percevejo_verde.24.jpg  \n",
            "  inflating: data/train/percevejo_verde.31.jpg  \n",
            "  inflating: data/train/percevejo_verde.25.jpg  \n",
            "  inflating: data/train/percevejo_verde.26.jpg  \n",
            "  inflating: data/train/percevejo_verde.27.jpg  \n",
            "  inflating: data/train/percevejo_verde.28.jpg  \n",
            "  inflating: data/train/percevejo_verde.8.jpg  \n",
            "  inflating: data/train/percevejo_verde.9.jpg  \n",
            "  inflating: data/train/percevejo_verde.10.jpg  \n",
            "  inflating: data/train/percevejo_verde.11.jpg  \n",
            "  inflating: data/train/percevejo_verde.12.jpg  \n",
            "  inflating: data/train/percevejo_verde.13.jpg  \n",
            "  inflating: data/train/percevejo_verde.14.jpg  \n",
            "  inflating: data/train/percevejo_verde.15.jpg  \n",
            "  inflating: data/train/percevejo_verde.16.jpg  \n",
            "  inflating: data/train/percevejo_verde.17.jpg  \n",
            "  inflating: data/train/percevejo_verde.18.jpg  \n",
            "  inflating: data/train/percevejo_verde.19.jpg  \n",
            "  inflating: data/train/percevejo_verde.29.jpg  \n",
            "  inflating: data/train/percevejo_verde.32.jpg  \n",
            "  inflating: data/train/percevejo_verde.33.jpg  \n",
            "  inflating: data/train/percevejo_verde.34.png  \n",
            "  inflating: data/train/percevejo_verde.35.png  \n",
            "  inflating: data/train/percevejo_verde.36.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKM9Rk0YYG-_",
        "outputId": "47940ea5-63b8-4929-90f1-3e6f3389ab4e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypVE0Y3cYW9H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCmYyH7NoDcS"
      },
      "source": [
        "!mkdir data/hdf5\n",
        "!mkdir data/output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KQjEjj79tCV5",
        "outputId": "75f2f687-c688-48d4-ba2e-bfc9915b20ad"
      },
      "source": [
        "ls data/train | wc"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    270     270    5195\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl7hYvPJo4y_"
      },
      "source": [
        "# define the paths to the images directory\n",
        "IMAGES_PATH = \"data/train\"\n",
        "\n",
        "# since we do not have validation data or access to the testing\n",
        "# labels we need to take a number of images from the training\n",
        "# data and use them instead\n",
        "NUM_CLASSES = 5\n",
        "NUM_VAL_IMAGES = 7 * NUM_CLASSES\n",
        "NUM_TEST_IMAGES = 7 * NUM_CLASSES\n",
        "\n",
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"data/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"data/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"data/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"data/cocamar.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"data/cocamar.json\"\n",
        "\n",
        "# define the path to the output directory used for storing plots,\n",
        "# classification reports, etc.\n",
        "OUTPUT_PATH = \"data/output\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omp0jxsm9Yr9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ad81c4d8-ab5b-4923-c200-eb402fdfdb25"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import json\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# grab the paths to the images\n",
        "trainPaths = list(paths.list_images(IMAGES_PATH))\n",
        "trainLabels = [p.split(os.path.sep)[-1].split(\".\")[0] for p in trainPaths]\n",
        "le = LabelEncoder()\n",
        "trainLabels = le.fit_transform(trainLabels)\n",
        "\n",
        "# perform stratified sampling from the training set to build the\n",
        "# testing split from the training data\n",
        "split = train_test_split(trainPaths, trainLabels,\n",
        "                         test_size=NUM_TEST_IMAGES, \n",
        "                         stratify=trainLabels,random_state=42)\n",
        "(trainPaths, testPaths, trainLabels, testLabels) = split\n",
        "\n",
        "# perform another stratified sampling, this time to build the\n",
        "# validation data\n",
        "split = train_test_split(trainPaths, trainLabels,\n",
        "                         test_size=NUM_VAL_IMAGES, \n",
        "                         stratify=trainLabels,random_state=42)\n",
        "(trainPaths, valPaths, trainLabels, valLabels) = split\n",
        "\n",
        "# construct a list pairing the training, validation, and testing\n",
        "# image paths along with their corresponding labels and output HDF5\n",
        "# files\n",
        "datasets = [\n",
        "\t(\"train\", trainPaths, trainLabels, TRAIN_HDF5),\n",
        "\t(\"val\", valPaths, valLabels, VAL_HDF5),\n",
        "\t(\"test\", testPaths, testLabels, TEST_HDF5)]\n",
        "\n",
        "# initialize the image pre-processor and the lists of RGB channel\n",
        "# averages\n",
        "aap = AspectAwarePreprocessor(256, 256)\n",
        "(R, G, B) = ([], [], [])\n",
        "\n",
        "# loop over the dataset tuples\n",
        "for (dType, paths, labels, outputPath) in datasets:\n",
        "\t# create HDF5 writer\n",
        "\tprint(\"[INFO] building {}...\".format(outputPath))\n",
        "\twriter = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath)\n",
        "\n",
        "\t# initialize the progress bar\n",
        "\twidgets = [\"Building Dataset: \", progressbar.Percentage(), \" \",progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "\tpbar = progressbar.ProgressBar(maxval=len(paths),widgets=widgets).start()\n",
        "\n",
        "\t# loop over the image paths\n",
        "\tfor (i, (path, label)) in enumerate(zip(paths, labels)):\n",
        "\t\t# load the image and process it\n",
        "\t\timage = cv2.imread(path)\n",
        "\t\timage = aap.preprocess(image)\n",
        "\n",
        "\t\t# if we are building the training dataset, then compute the\n",
        "\t\t# mean of each channel in the image, then update the\n",
        "\t\t# respective lists\n",
        "\t\tif dType == \"train\":\n",
        "\t\t\t(b, g, r) = cv2.mean(image)[:3]\n",
        "\t\t\tR.append(r)\n",
        "\t\t\tG.append(g)\n",
        "\t\t\tB.append(b)\n",
        "\n",
        "\t\t# add the image and label # to the HDF5 dataset\n",
        "\t\twriter.add([image], [label])\n",
        "\t\tpbar.update(i)\n",
        "\n",
        "\t# close the HDF5 writer\n",
        "\tpbar.finish()\n",
        "\twriter.close()\n",
        "\n",
        "# construct a dictionary of averages, then serialize the means to a\n",
        "# JSON file\n",
        "print(\"[INFO] serializing means...\")\n",
        "D = {\"R\": np.mean(R), \"G\": np.mean(G), \"B\": np.mean(B)}\n",
        "f = open(DATASET_MEAN, \"w\")\n",
        "f.write(json.dumps(D))\n",
        "f.close()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Dataset:   2% |#                                      | ETA:   0:00:05"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] building data/hdf5/train.hdf5...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building Dataset: 100% |#######################################| Time:  0:00:01\n",
            "Building Dataset:  51% |####################                   | ETA:   0:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] building data/hdf5/val.hdf5...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building Dataset: 100% |#######################################| Time:  0:00:00\n",
            "Building Dataset:  60% |#######################                | ETA:   0:00:00"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] building data/hdf5/test.hdf5...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building Dataset: 100% |#######################################| Time:  0:00:00\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] serializing means...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HkXIZpCslRci",
        "outputId": "036c556c-e821-4d90-8f7b-6a7a5a3ba9ea"
      },
      "source": [
        "!du -h"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.0K\t./.config/configurations\n",
            "60K\t./.config/logs/2020.12.02\n",
            "64K\t./.config/logs\n",
            "100K\t./.config\n",
            "56M\t./data/hdf5\n",
            "31M\t./data/train\n",
            "4.0K\t./data/output\n",
            "87M\t./data\n",
            "55M\t./sample_data\n",
            "171M\t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFkeXcghlU6a"
      },
      "source": [
        "# 3.0 Competing in Kaggle: Dogs vs. Cats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFt0U2oUmMuU"
      },
      "source": [
        "## 3.1 Implementing AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax0EAsPgmEea"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class AlexNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes, reg=0.0002):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\" and the channels dimension itself\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\t\tchanDim = -1\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\t# and channels dimension\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\t\t\tchanDim = 1\n",
        "\n",
        "\t\t# Block #1: first CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(96, (11, 11), strides=(4, 4),input_shape=inputShape, padding=\"valid\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #2: second CONV => RELU => POOL layer set\n",
        "\t\tmodel.add(Conv2D(256, (5, 5), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #3: CONV => RELU => CONV => RELU => CONV => RELU\n",
        "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(Conv2D(256, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\t# Block #4: first set of FC => RELU layers\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# Block #5: second set of FC => RELU layers\n",
        "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\tmodel.add(BatchNormalization())\n",
        "\t\tmodel.add(Dropout(0.5))\n",
        "\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Dense(classes, kernel_regularizer=l2(reg)))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeWP4j7-nGuU"
      },
      "source": [
        "## 3.2 Training AlexNet on Kaggle: dogs vs cats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXM6nqiaqBh1"
      },
      "source": [
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"data/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"data/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"data/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"data/cocamar.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"data/cocamar.json\"\n",
        "\n",
        "# define the path to the output directory used for storing plots,\n",
        "# classification reports, etc.\n",
        "OUTPUT_PATH = \"cocamar/\"\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MmNyJTrMpfeY",
        "outputId": "1129d43a-1d08-4dac-def2-0eb0aa83839c"
      },
      "source": [
        "# import the necessary packages\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configurations related to checkpoint\n",
        "# resume or not the model\n",
        "resume = False\n",
        "\n",
        "# checkpoint files\n",
        "filepath= \"cocamar/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                         width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,\n",
        "                         shear_range=0.15,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())\n",
        "\n",
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(227, 227)\n",
        "pp = PatchPreprocessor(227, 227)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=5)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=5)\n",
        "\n",
        "# initialize the optimizer\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=1e-3)\n",
        "\n",
        "model = AlexNet.build(width=227, height=227, depth=3,classes=5, reg=0.0002)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "# construct the set of callbacks\n",
        "path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\n",
        "callbacks = [TrainingMonitor(path)]\n",
        "\n",
        "initial_epoch = 1\n",
        "\n",
        "# load previou weights\n",
        "if resume == True:\n",
        "  model.load_weights(\"cocamar/epochs:009-val_acc:0.743.hdf5\")\n",
        "  initial_epoch = 45\n",
        "\n",
        "# train the network\n",
        "history = model.fit(trainGen.generator(),\n",
        "          steps_per_epoch=trainGen.numImages // 2,\n",
        "          validation_data=valGen.generator(),\n",
        "          validation_steps=valGen.numImages // 2,\n",
        "          epochs=100,\n",
        "          max_queue_size=10,\n",
        "          callbacks=[callbacks,checkpoint], verbose=1,\n",
        "          initial_epoch=initial_epoch)\n",
        "\n",
        "# save the model to file\n",
        "print(\"[INFO] serializing model...\")\n",
        "model.save(MODEL_PATH, overwrite=True)\n",
        "\n",
        "# close the HDF5 datasets\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n",
            "Epoch 2/100\n",
            "97/97 [==============================] - 171s 2s/step - loss: 3.1882 - accuracy: 0.4971 - val_loss: 4.0719 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00002: val_accuracy improved from -inf to 0.51429, saving model to cocamar/epochs:002-val_acc:0.514.hdf5\n",
            "Epoch 3/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 2.0929 - accuracy: 0.9520 - val_loss: 2.3154 - val_accuracy: 0.4571\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.51429\n",
            "Epoch 4/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 1.5241 - accuracy: 0.9806 - val_loss: 1.7671 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.51429 to 0.65714, saving model to cocamar/epochs:004-val_acc:0.657.hdf5\n",
            "Epoch 5/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 1.1383 - accuracy: 0.9747 - val_loss: 1.8866 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.65714\n",
            "Epoch 6/100\n",
            "97/97 [==============================] - 159s 2s/step - loss: 0.8836 - accuracy: 0.9836 - val_loss: 1.6771 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.65714\n",
            "Epoch 7/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.7190 - accuracy: 0.9864 - val_loss: 1.2820 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.65714\n",
            "Epoch 8/100\n",
            "97/97 [==============================] - 159s 2s/step - loss: 0.5184 - accuracy: 0.9937 - val_loss: 1.2661 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.65714\n",
            "Epoch 9/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.5156 - accuracy: 0.9783 - val_loss: 0.9284 - val_accuracy: 0.7429\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.65714 to 0.74286, saving model to cocamar/epochs:009-val_acc:0.743.hdf5\n",
            "Epoch 10/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.3918 - accuracy: 0.9927 - val_loss: 1.3730 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.74286\n",
            "Epoch 11/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.3750 - accuracy: 0.9943 - val_loss: 0.8805 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.74286\n",
            "Epoch 12/100\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.3727 - accuracy: 0.9772 - val_loss: 1.3081 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.74286\n",
            "Epoch 13/100\n",
            "97/97 [==============================] - 164s 2s/step - loss: 0.3716 - accuracy: 0.9956 - val_loss: 0.9027 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.74286\n",
            "Epoch 14/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.2851 - accuracy: 0.9890 - val_loss: 1.0203 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.74286\n",
            "Epoch 15/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.2692 - accuracy: 0.9831 - val_loss: 1.3344 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.74286\n",
            "Epoch 16/100\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.2766 - accuracy: 0.9835 - val_loss: 1.1780 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.74286\n",
            "Epoch 17/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.1997 - accuracy: 0.9982 - val_loss: 0.9908 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.74286\n",
            "Epoch 18/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.1880 - accuracy: 0.9919 - val_loss: 1.5134 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.74286\n",
            "Epoch 19/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.3115 - accuracy: 0.9820 - val_loss: 0.9673 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.74286\n",
            "Epoch 20/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.2709 - accuracy: 0.9862 - val_loss: 0.8257 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.74286\n",
            "Epoch 21/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.1832 - accuracy: 0.9949 - val_loss: 0.8062 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.74286\n",
            "Epoch 22/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.2306 - accuracy: 0.9883 - val_loss: 1.3158 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.74286\n",
            "Epoch 23/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.2297 - accuracy: 0.9871 - val_loss: 1.2260 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.74286\n",
            "Epoch 24/100\n",
            "97/97 [==============================] - 158s 2s/step - loss: 0.2394 - accuracy: 0.9851 - val_loss: 1.2564 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.74286\n",
            "Epoch 25/100\n",
            "97/97 [==============================] - 165s 2s/step - loss: 0.2304 - accuracy: 0.9863 - val_loss: 1.7807 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.74286\n",
            "Epoch 26/100\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.2875 - accuracy: 0.9864 - val_loss: 0.9594 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.74286\n",
            "Epoch 27/100\n",
            "97/97 [==============================] - 167s 2s/step - loss: 0.3140 - accuracy: 0.9835 - val_loss: 1.0771 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.74286\n",
            "Epoch 28/100\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.2354 - accuracy: 0.9885 - val_loss: 0.9812 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.74286\n",
            "Epoch 29/100\n",
            "97/97 [==============================] - 168s 2s/step - loss: 0.1969 - accuracy: 0.9923 - val_loss: 2.0620 - val_accuracy: 0.4571\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.74286\n",
            "Epoch 30/100\n",
            "97/97 [==============================] - 167s 2s/step - loss: 0.2617 - accuracy: 0.9853 - val_loss: 1.6568 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.74286\n",
            "Epoch 31/100\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.2653 - accuracy: 0.9849 - val_loss: 1.1524 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.74286\n",
            "Epoch 32/100\n",
            "97/97 [==============================] - 167s 2s/step - loss: 0.3278 - accuracy: 0.9823 - val_loss: 1.3846 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.74286\n",
            "Epoch 33/100\n",
            "97/97 [==============================] - 167s 2s/step - loss: 0.2609 - accuracy: 0.9895 - val_loss: 1.2398 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.74286\n",
            "Epoch 34/100\n",
            "97/97 [==============================] - 164s 2s/step - loss: 0.2890 - accuracy: 0.9911 - val_loss: 1.1131 - val_accuracy: 0.4571\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.74286\n",
            "Epoch 35/100\n",
            "97/97 [==============================] - 167s 2s/step - loss: 0.2073 - accuracy: 0.9927 - val_loss: 1.7130 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.74286\n",
            "Epoch 36/100\n",
            "97/97 [==============================] - 173s 2s/step - loss: 0.2247 - accuracy: 0.9924 - val_loss: 1.3144 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.74286\n",
            "Epoch 37/100\n",
            "97/97 [==============================] - 174s 2s/step - loss: 0.2596 - accuracy: 0.9854 - val_loss: 1.7292 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.74286\n",
            "Epoch 38/100\n",
            "97/97 [==============================] - 173s 2s/step - loss: 0.2730 - accuracy: 0.9871 - val_loss: 1.8641 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.74286\n",
            "Epoch 39/100\n",
            "97/97 [==============================] - 175s 2s/step - loss: 0.3484 - accuracy: 0.9862 - val_loss: 1.1551 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.74286\n",
            "Epoch 40/100\n",
            "97/97 [==============================] - 172s 2s/step - loss: 0.2573 - accuracy: 0.9911 - val_loss: 1.1030 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.74286\n",
            "Epoch 41/100\n",
            "97/97 [==============================] - 174s 2s/step - loss: 0.2383 - accuracy: 0.9917 - val_loss: 1.5411 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.74286\n",
            "Epoch 42/100\n",
            "97/97 [==============================] - 174s 2s/step - loss: 0.2899 - accuracy: 0.9880 - val_loss: 1.6983 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.74286\n",
            "Epoch 43/100\n",
            "97/97 [==============================] - 172s 2s/step - loss: 0.2400 - accuracy: 0.9909 - val_loss: 1.4608 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.74286\n",
            "Epoch 44/100\n",
            "97/97 [==============================] - 170s 2s/step - loss: 0.2369 - accuracy: 0.9935 - val_loss: 1.1478 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.74286\n",
            "Epoch 45/100\n",
            "97/97 [==============================] - 171s 2s/step - loss: 0.3266 - accuracy: 0.9903 - val_loss: 0.9703 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.74286\n",
            "Epoch 46/100\n",
            "97/97 [==============================] - 171s 2s/step - loss: 0.2584 - accuracy: 0.9877 - val_loss: 1.3079 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.74286\n",
            "Epoch 47/100\n",
            "97/97 [==============================] - 173s 2s/step - loss: 0.3368 - accuracy: 0.9799 - val_loss: 1.5520 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.74286\n",
            "Epoch 48/100\n",
            "97/97 [==============================] - 172s 2s/step - loss: 0.2847 - accuracy: 0.9865 - val_loss: 0.9473 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.74286\n",
            "Epoch 49/100\n",
            "97/97 [==============================] - 168s 2s/step - loss: 0.2794 - accuracy: 0.9883 - val_loss: 1.0663 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.74286\n",
            "Epoch 50/100\n",
            "97/97 [==============================] - 165s 2s/step - loss: 0.2548 - accuracy: 0.9899 - val_loss: 1.9344 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.74286\n",
            "Epoch 51/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.3185 - accuracy: 0.9878 - val_loss: 1.3969 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.74286\n",
            "Epoch 52/100\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.2337 - accuracy: 0.9920 - val_loss: 1.3541 - val_accuracy: 0.4571\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.74286\n",
            "Epoch 53/100\n",
            "97/97 [==============================] - 166s 2s/step - loss: 0.2154 - accuracy: 0.9917 - val_loss: 1.6901 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.74286\n",
            "Epoch 54/100\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.2612 - accuracy: 0.9926 - val_loss: 1.1512 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.74286\n",
            "Epoch 55/100\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.2785 - accuracy: 0.9903 - val_loss: 1.5200 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.74286\n",
            "Epoch 56/100\n",
            "97/97 [==============================] - 166s 2s/step - loss: 0.2643 - accuracy: 0.9886 - val_loss: 1.0828 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.74286\n",
            "Epoch 57/100\n",
            "97/97 [==============================] - 168s 2s/step - loss: 0.3440 - accuracy: 0.9807 - val_loss: 1.5756 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.74286\n",
            "Epoch 58/100\n",
            "97/97 [==============================] - 168s 2s/step - loss: 0.3174 - accuracy: 0.9879 - val_loss: 1.1717 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.74286\n",
            "Epoch 59/100\n",
            "97/97 [==============================] - 168s 2s/step - loss: 0.3290 - accuracy: 0.9859 - val_loss: 1.3885 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.74286\n",
            "Epoch 60/100\n",
            "97/97 [==============================] - 168s 2s/step - loss: 0.2756 - accuracy: 0.9915 - val_loss: 1.6318 - val_accuracy: 0.4571\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.74286\n",
            "Epoch 61/100\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.2896 - accuracy: 0.9896 - val_loss: 1.1635 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.74286\n",
            "Epoch 62/100\n",
            "97/97 [==============================] - 167s 2s/step - loss: 0.2650 - accuracy: 0.9918 - val_loss: 0.7819 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.74286\n",
            "Epoch 63/100\n",
            "97/97 [==============================] - 166s 2s/step - loss: 0.1948 - accuracy: 0.9924 - val_loss: 1.2869 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.74286\n",
            "Epoch 64/100\n",
            "97/97 [==============================] - 166s 2s/step - loss: 0.2594 - accuracy: 0.9884 - val_loss: 1.2486 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.74286\n",
            "Epoch 65/100\n",
            "97/97 [==============================] - 168s 2s/step - loss: 0.3021 - accuracy: 0.9884 - val_loss: 1.1626 - val_accuracy: 0.4571\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.74286\n",
            "Epoch 66/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.2120 - accuracy: 0.9930 - val_loss: 6.0326 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.74286\n",
            "Epoch 67/100\n",
            "97/97 [==============================] - 164s 2s/step - loss: 0.2904 - accuracy: 0.9868 - val_loss: 0.9951 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.74286\n",
            "Epoch 68/100\n",
            "97/97 [==============================] - 164s 2s/step - loss: 0.2395 - accuracy: 0.9925 - val_loss: 1.2402 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.74286\n",
            "Epoch 69/100\n",
            "97/97 [==============================] - 164s 2s/step - loss: 0.2392 - accuracy: 0.9893 - val_loss: 1.3838 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.74286\n",
            "Epoch 70/100\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.2696 - accuracy: 0.9883 - val_loss: 1.0805 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.74286\n",
            "Epoch 71/100\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.2709 - accuracy: 0.9880 - val_loss: 1.3003 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.74286\n",
            "Epoch 72/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.2492 - accuracy: 0.9884 - val_loss: 1.0986 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.74286\n",
            "Epoch 73/100\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.2225 - accuracy: 0.9941 - val_loss: 1.5599 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.74286\n",
            "Epoch 74/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.2404 - accuracy: 0.9910 - val_loss: 1.0317 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.74286\n",
            "Epoch 75/100\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.2407 - accuracy: 0.9913 - val_loss: 0.9846 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.74286\n",
            "Epoch 76/100\n",
            "97/97 [==============================] - 170s 2s/step - loss: 0.2383 - accuracy: 0.9874 - val_loss: 1.2769 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.74286\n",
            "Epoch 77/100\n",
            "97/97 [==============================] - 165s 2s/step - loss: 0.2863 - accuracy: 0.9855 - val_loss: 1.5330 - val_accuracy: 0.4286\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.74286\n",
            "Epoch 78/100\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.2301 - accuracy: 0.9931 - val_loss: 1.1248 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.74286\n",
            "Epoch 79/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.2368 - accuracy: 0.9895 - val_loss: 1.5615 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.74286\n",
            "Epoch 80/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.1892 - accuracy: 0.9944 - val_loss: 1.8052 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.74286\n",
            "Epoch 81/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.2102 - accuracy: 0.9928 - val_loss: 1.8029 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.74286\n",
            "Epoch 82/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.2232 - accuracy: 0.9917 - val_loss: 1.0243 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.74286\n",
            "Epoch 83/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.1915 - accuracy: 0.9921 - val_loss: 0.8797 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.74286\n",
            "Epoch 84/100\n",
            "97/97 [==============================] - 164s 2s/step - loss: 0.1908 - accuracy: 0.9958 - val_loss: 1.4455 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.74286\n",
            "Epoch 85/100\n",
            "97/97 [==============================] - 165s 2s/step - loss: 0.2098 - accuracy: 0.9898 - val_loss: 1.1010 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.74286\n",
            "Epoch 86/100\n",
            "97/97 [==============================] - 165s 2s/step - loss: 0.2490 - accuracy: 0.9929 - val_loss: 1.1214 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.74286\n",
            "Epoch 87/100\n",
            "97/97 [==============================] - 171s 2s/step - loss: 0.2419 - accuracy: 0.9884 - val_loss: 0.9133 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.74286\n",
            "Epoch 88/100\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.2080 - accuracy: 0.9923 - val_loss: 1.0673 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.74286\n",
            "Epoch 89/100\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.1964 - accuracy: 0.9927 - val_loss: 0.8974 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.74286\n",
            "Epoch 90/100\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.2057 - accuracy: 0.9922 - val_loss: 1.1749 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.74286\n",
            "Epoch 91/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.1970 - accuracy: 0.9954 - val_loss: 1.4760 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.74286\n",
            "Epoch 92/100\n",
            "97/97 [==============================] - 159s 2s/step - loss: 0.2879 - accuracy: 0.9803 - val_loss: 0.7692 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.74286\n",
            "Epoch 93/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.1755 - accuracy: 0.9955 - val_loss: 1.0684 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.74286\n",
            "Epoch 94/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.2181 - accuracy: 0.9912 - val_loss: 1.4504 - val_accuracy: 0.4571\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.74286\n",
            "Epoch 95/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.2366 - accuracy: 0.9901 - val_loss: 0.9397 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.74286\n",
            "Epoch 96/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.1686 - accuracy: 0.9954 - val_loss: 0.7239 - val_accuracy: 0.6857\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.74286\n",
            "Epoch 97/100\n",
            "97/97 [==============================] - 161s 2s/step - loss: 0.1919 - accuracy: 0.9938 - val_loss: 1.2247 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.74286\n",
            "Epoch 98/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.1711 - accuracy: 0.9924 - val_loss: 1.2198 - val_accuracy: 0.4857\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.74286\n",
            "Epoch 99/100\n",
            "97/97 [==============================] - 160s 2s/step - loss: 0.2225 - accuracy: 0.9922 - val_loss: 1.3409 - val_accuracy: 0.5143\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.74286\n",
            "Epoch 100/100\n",
            "97/97 [==============================] - 159s 2s/step - loss: 0.1921 - accuracy: 0.9932 - val_loss: 1.0336 - val_accuracy: 0.5429\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.74286\n",
            "[INFO] serializing model...\n",
            "INFO:tensorflow:Assets written to: data/cocamar.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgP_SX7JBvSK"
      },
      "source": [
        "## 3.3 Evaluating AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A8nruX7p86lq",
        "outputId": "90980b22-96a4-46e7-d26f-603babd5bd7b"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import json\n",
        "\n",
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())\n",
        "\n",
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(227, 227)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "cp = CropPreprocessor(227, 227)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# load the pretrained network\n",
        "print(\"[INFO] loading model...\")\n",
        "model = load_model(MODEL_PATH)\n",
        "\n",
        "# initialize the testing dataset generator, then make predictions on\n",
        "# the testing data\n",
        "print(\"[INFO] predicting on test data (no crops)...\")\n",
        "testGen = HDF5DatasetGenerator(TEST_HDF5, 64,\n",
        "                               preprocessors=[sp, mp, iap], classes=5)\n",
        "predictions = model.predict(testGen.generator(),\n",
        "                            steps=testGen.numImages // 8, max_queue_size=10)\n",
        "\n",
        "# compute the rank-1 and rank-5 accuracies\n",
        "(rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n",
        "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n",
        "testGen.close()\n",
        "\n",
        "\n",
        "# re-initialize the testing set generator, this time excluding the\n",
        "# `SimplePreprocessor`\n",
        "testGen = HDF5DatasetGenerator(TEST_HDF5, 64,\n",
        "                               preprocessors=[mp], classes=5)\n",
        "predictions = []\n",
        "\n",
        "# initialize the progress bar\n",
        "widgets = [\"Evaluating: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "pbar = progressbar.ProgressBar(maxval=testGen.numImages // 64,widgets=widgets).start()\n",
        "\n",
        "# loop over a single pass of the test data\n",
        "for (i, (images, labels)) in enumerate(testGen.generator(passes=1)):\n",
        "\t# loop over each of the individual images\n",
        "\tfor image in images:\n",
        "\t\t# apply the crop preprocessor to the image to generate 10\n",
        "\t\t# separate crops, then convert them from images to arrays\n",
        "\t\tcrops = cp.preprocess(image)\n",
        "\t\tcrops = np.array([iap.preprocess(c) for c in crops],\n",
        "\t\t\tdtype=\"float32\")\n",
        "\n",
        "\t\t# make predictions on the crops and then average them\n",
        "\t\t# together to obtain the final prediction\n",
        "\t\tpred = model.predict(crops)\n",
        "\t\tpredictions.append(pred.mean(axis=0))\n",
        "\n",
        "\t# update the progress bar\n",
        "\tpbar.update(i)\n",
        "\n",
        "# compute the rank-1 accuracy\n",
        "pbar.finish()\n",
        "print(\"[INFO] predicting on test data (with crops)...\")\n",
        "(rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n",
        "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n",
        "testGen.close()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading model...\n",
            "[INFO] predicting on test data (no crops)...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rEvaluating: 100% |#                                            | ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] rank-1: 15.71%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100% |#                                            | ETA:  --:--:--\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] predicting on test data (with crops)...\n",
            "[INFO] rank-1: 68.57%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFwa9y74MEhy"
      },
      "source": [
        "## 3.4 Obtaining the top spot on the Kaggle Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF_lpAJ7Nm7s"
      },
      "source": [
        "Of course, if you were to look at the Kaggle Dogs vs. Cats leaderboard, you would notice that to even break into the top-25 position we would need 96.69% accuracy, which our current method is not capable of reaching. So, what’s the solution?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8boDWfTt6a97"
      },
      "source": [
        "### 3.4.1 Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s57VlfPY6aLf"
      },
      "source": [
        "\n",
        "\n",
        "The answer is **transfer learning**, specifically transfer learning via **feature extraction**. While the ImageNet dataset consists of 1,000 object categories, a good portion of those include both dog species and cat species. Therefore, a network trained on ImageNet could not only tell you if an image was of a dog or a cat, but what particular breed the animal is as well. \n",
        "\n",
        "Given that a network trained on ImageNet must be able to discriminate between such fine-grained animals, it’s natural to hypothesize that the features extracted from a pre-trained network would likely lend itself well to claiming a top spot on the Kaggle Dogs vs. Cats leaderboard.\n",
        "\n",
        "> To test this hypothesis, let’s first extract features from the **pre-trained ResNet** architecture and then train a Logistic Regression classifier on top of these features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msl4nCWDS6_R"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import h5py\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvV7V17bUGSY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "98220428-86ed-46ee-886e-9e121307363b"
      },
      "source": [
        "# whether or not to include top of CNN\n",
        "include_top = 1\n",
        "\n",
        "# load the Resnet50 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = ResNet50(weights=\"imagenet\", include_top= include_top > 0)\n",
        "print(\"[INFO] showing layers...\")\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102973440/102967424 [==============================] - 1s 0us/step\n",
            "[INFO] showing layers...\n",
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 25,636,712\n",
            "Trainable params: 25,583,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3SxB5qoRX20"
      },
      "source": [
        "def feature_extraction(dataset,output,buffer_size,bs):\n",
        "\t\t'''\n",
        "\t\t\tdataset: input folder with images dataset\n",
        "\t\t\toutput: folder to store the feature extraction\n",
        "\t\t\tbuffer_size: controls the size of our in-memory buffer\n",
        "\t\t\tbs: batch size\n",
        "\t\t'''\n",
        "\n",
        "\t\t# grab the list of images that we'll be describing then randomly\n",
        "\t\t# shuffle them to allow for easy training and testing splits via\n",
        "\t\t# array slicing during training time\n",
        "\t\tprint(\"[INFO] loading images...\")\n",
        "\t\timagePaths = list(paths.list_images(dataset))\n",
        "\t\trandom.shuffle(imagePaths)\n",
        "\n",
        "\t\t# extract the class labels from the image paths then encode the\n",
        "\t\t# labels\n",
        "\t\tlabels = [p.split(os.path.sep)[-1].split(\".\")[0] for p in imagePaths]\n",
        "\t\tle = LabelEncoder()\n",
        "\t\tlabels = le.fit_transform(labels)\t\n",
        "\n",
        "\t\t# load the VGG16 network\n",
        "\t\tprint(\"[INFO] loading network...\")\n",
        "\t\tmodel = ResNet50(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "\t\t# initialize the HDF5 dataset writer, then store the class label\n",
        "\t\t# names in the dataset\n",
        "\t\tdataset = HDF5DatasetWriter((len(imagePaths), 7*7*2048),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toutput, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdataKey=\"features\", \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbufSize=buffer_size)\n",
        "\t\tdataset.storeClassLabels(le.classes_)\n",
        "\n",
        "\t\t# initialize the progress bar\n",
        "\t\twidgets = [\"Extracting Features: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
        "\t\tpbar = progressbar.ProgressBar(maxval=len(imagePaths),widgets=widgets).start()\n",
        "\n",
        "\t\t# loop over the images in batches\n",
        "\t\tfor i in np.arange(0, len(imagePaths), bs):\n",
        "\t\t\t# extract the batch of images and labels, then initialize the\n",
        "\t\t\t# list of actual images that will be passed through the network\n",
        "\t\t\t# for feature extraction\n",
        "\t\t\tbatchPaths = imagePaths[i:i + bs]\n",
        "\t\t\tbatchLabels = labels[i:i + bs]\n",
        "\t\t\tbatchImages = []\n",
        "\n",
        "\t\t\t# loop over the images and labels in the current batch\n",
        "\t\t\tfor (j, imagePath) in enumerate(batchPaths):\n",
        "\t\t\t\t# load the input image using the Keras helper utility\n",
        "\t\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
        "\t\t\t\timage = load_img(imagePath, target_size=(224, 224))\n",
        "\t\t\t\timage = img_to_array(image)\n",
        "\n",
        "\t\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
        "\t\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
        "\t\t\t\t# ImageNet dataset\n",
        "\t\t\t\timage = np.expand_dims(image, axis=0)\n",
        "\t\t\t\timage = imagenet_utils.preprocess_input(image)\n",
        "\n",
        "\t\t\t\t# add the image to the batch\n",
        "\t\t\t\tbatchImages.append(image)\n",
        "\n",
        "\t\t\t# pass the images through the network and use the outputs as\n",
        "\t\t\t# our actual features\n",
        "\t\t\tbatchImages = np.vstack(batchImages)\n",
        "\t\t\tfeatures = model.predict(batchImages, batch_size=bs)\n",
        "\n",
        "\t\t\t# reshape the features so that each image is represented by\n",
        "\t\t\t# a flattened feature vector of the `MaxPooling2D` outputs\n",
        "\t\t\tfeatures = features.reshape((features.shape[0], 7*7*2048))\n",
        "\n",
        "\t\t\t# add the features and labels to our HDF5 dataset\n",
        "\t\t\tdataset.add(features, batchLabels)\n",
        "\t\t\tpbar.update(i)\n",
        "\n",
        "\t\t# close the dataset\n",
        "\t\tdataset.close()\n",
        "\t\tpbar.finish()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaCYgeLwWqVf"
      },
      "source": [
        "# INPUTS\n",
        "# path to input dataset\n",
        "dataset = \"data/train\"\n",
        "\n",
        "# path to output HDF5 file\n",
        "output  = \"data/hdf5/features.hdf5\"\n",
        "\n",
        "# size of feature extraction buffer\n",
        "buffer_size = 1000\n",
        "\n",
        "# store the batch size in a convenience variable\n",
        "bs = 32"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzL3B2kmW4jH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ed968737-5baa-416d-8250-25cd4aedc454"
      },
      "source": [
        "feature_extraction(dataset,output,buffer_size,bs)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] loading network...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting Features: 100% |####################################| Time:  0:00:06\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUQCvuxHSRcL"
      },
      "source": [
        "# import the necessary packages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "import h5py\n",
        "\n",
        "def train_and_evaluate(features_set):\n",
        "    db = h5py.File(features_set,mode='r')\n",
        "    print(\"Database keys {0:}\".format(list(db.keys())))\n",
        "\n",
        "    # open the HDF5 database for reading then determine the index of\n",
        "    # the training and testing split, provided that this data was\n",
        "    # already shuffled *prior* to writing it to disk\n",
        "    i = int(db[\"labels\"].shape[0] * 0.75)\n",
        "\n",
        "    # define the set of parameters that we want to tune then start a\n",
        "    # grid search where we evaluate our model for each value of C\n",
        "    print(\"[INFO] tuning hyperparameters...\")\n",
        "    params = {\"C\": [0.0001]}\n",
        "    model = GridSearchCV(LogisticRegression(solver=\"lbfgs\",multi_class=\"auto\"),params, cv=3)\n",
        "\n",
        "    model.fit(db[\"features\"][:i], db[\"labels\"][:i])\n",
        "    print(\"[INFO] best hyperparameters: {}\".format(model.best_params_))\n",
        "\n",
        "    # evaluate the model\n",
        "    print(\"[INFO] evaluating...\")\n",
        "    preds = model.predict(db[\"features\"][i:])\n",
        "    print(classification_report(db[\"labels\"][i:], \n",
        "                                preds,\n",
        "                                target_names=db[\"label_names\"]))\n",
        "\n",
        "    # serialize the model to disk\n",
        "    print(\"[INFO] saving model...\")\n",
        "    f = open(features_set.split(\"/\")[0] + \".cpickle\", \"wb\")\n",
        "    f.write(pickle.dumps(model.best_estimator_))\n",
        "    f.close()\n",
        "\n",
        "    # close the database\n",
        "    db.close()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK6T2vWmY9b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1dae10d4-e419-4677-d934-3b130ef7428e"
      },
      "source": [
        "train_and_evaluate(output)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database keys ['features', 'label_names', 'labels']\n",
            "[INFO] tuning hyperparameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] best hyperparameters: {'C': 0.0001}\n",
            "[INFO] evaluating...\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "          lagarta       1.00      0.88      0.93         8\n",
            "         negative       0.80      1.00      0.89        32\n",
            " percevejo_marrom       1.00      0.76      0.87        17\n",
            "percevejo_pequeno       1.00      1.00      1.00         2\n",
            "  percevejo_verde       0.80      0.50      0.62         8\n",
            "\n",
            "         accuracy                           0.87        67\n",
            "        macro avg       0.92      0.83      0.86        67\n",
            "     weighted avg       0.88      0.87      0.86        67\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcfAFunl6jY0"
      },
      "source": [
        "### 3.4.2 Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lequt3Gn7G0B"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import progressbar\n",
        "import h5py\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQFxYXJ6RSrO"
      },
      "source": [
        "# define the path to the output training, validation, and testing\n",
        "# HDF5 files\n",
        "TRAIN_HDF5 = \"data/hdf5/train.hdf5\"\n",
        "VAL_HDF5 = \"data/hdf5/val.hdf5\"\n",
        "TEST_HDF5 = \"data/hdf5/test.hdf5\"\n",
        "\n",
        "# path to the output model file\n",
        "MODEL_PATH = \"data/cocamar.model\"\n",
        "\n",
        "# define the path to the dataset mean\n",
        "DATASET_MEAN = \"data/cocamar.json\""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm87KJ537Ead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036e1aa6-b4c0-4db5-f836-e2abd190cd1e"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# whether or not to include top of CNN\n",
        "include_top = 0\n",
        "\n",
        "# load the ResNet50 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = ResNet50(weights=\"imagenet\", include_top= include_top > 0)\n",
        "print(\"[INFO] showing layers...\")\n",
        "\n",
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading network...\n",
            "[INFO] showing layers...\n",
            "[INFO] 0\tInputLayer\n",
            "[INFO] 1\tZeroPadding2D\n",
            "[INFO] 2\tConv2D\n",
            "[INFO] 3\tBatchNormalization\n",
            "[INFO] 4\tActivation\n",
            "[INFO] 5\tZeroPadding2D\n",
            "[INFO] 6\tMaxPooling2D\n",
            "[INFO] 7\tConv2D\n",
            "[INFO] 8\tBatchNormalization\n",
            "[INFO] 9\tActivation\n",
            "[INFO] 10\tConv2D\n",
            "[INFO] 11\tBatchNormalization\n",
            "[INFO] 12\tActivation\n",
            "[INFO] 13\tConv2D\n",
            "[INFO] 14\tConv2D\n",
            "[INFO] 15\tBatchNormalization\n",
            "[INFO] 16\tBatchNormalization\n",
            "[INFO] 17\tAdd\n",
            "[INFO] 18\tActivation\n",
            "[INFO] 19\tConv2D\n",
            "[INFO] 20\tBatchNormalization\n",
            "[INFO] 21\tActivation\n",
            "[INFO] 22\tConv2D\n",
            "[INFO] 23\tBatchNormalization\n",
            "[INFO] 24\tActivation\n",
            "[INFO] 25\tConv2D\n",
            "[INFO] 26\tBatchNormalization\n",
            "[INFO] 27\tAdd\n",
            "[INFO] 28\tActivation\n",
            "[INFO] 29\tConv2D\n",
            "[INFO] 30\tBatchNormalization\n",
            "[INFO] 31\tActivation\n",
            "[INFO] 32\tConv2D\n",
            "[INFO] 33\tBatchNormalization\n",
            "[INFO] 34\tActivation\n",
            "[INFO] 35\tConv2D\n",
            "[INFO] 36\tBatchNormalization\n",
            "[INFO] 37\tAdd\n",
            "[INFO] 38\tActivation\n",
            "[INFO] 39\tConv2D\n",
            "[INFO] 40\tBatchNormalization\n",
            "[INFO] 41\tActivation\n",
            "[INFO] 42\tConv2D\n",
            "[INFO] 43\tBatchNormalization\n",
            "[INFO] 44\tActivation\n",
            "[INFO] 45\tConv2D\n",
            "[INFO] 46\tConv2D\n",
            "[INFO] 47\tBatchNormalization\n",
            "[INFO] 48\tBatchNormalization\n",
            "[INFO] 49\tAdd\n",
            "[INFO] 50\tActivation\n",
            "[INFO] 51\tConv2D\n",
            "[INFO] 52\tBatchNormalization\n",
            "[INFO] 53\tActivation\n",
            "[INFO] 54\tConv2D\n",
            "[INFO] 55\tBatchNormalization\n",
            "[INFO] 56\tActivation\n",
            "[INFO] 57\tConv2D\n",
            "[INFO] 58\tBatchNormalization\n",
            "[INFO] 59\tAdd\n",
            "[INFO] 60\tActivation\n",
            "[INFO] 61\tConv2D\n",
            "[INFO] 62\tBatchNormalization\n",
            "[INFO] 63\tActivation\n",
            "[INFO] 64\tConv2D\n",
            "[INFO] 65\tBatchNormalization\n",
            "[INFO] 66\tActivation\n",
            "[INFO] 67\tConv2D\n",
            "[INFO] 68\tBatchNormalization\n",
            "[INFO] 69\tAdd\n",
            "[INFO] 70\tActivation\n",
            "[INFO] 71\tConv2D\n",
            "[INFO] 72\tBatchNormalization\n",
            "[INFO] 73\tActivation\n",
            "[INFO] 74\tConv2D\n",
            "[INFO] 75\tBatchNormalization\n",
            "[INFO] 76\tActivation\n",
            "[INFO] 77\tConv2D\n",
            "[INFO] 78\tBatchNormalization\n",
            "[INFO] 79\tAdd\n",
            "[INFO] 80\tActivation\n",
            "[INFO] 81\tConv2D\n",
            "[INFO] 82\tBatchNormalization\n",
            "[INFO] 83\tActivation\n",
            "[INFO] 84\tConv2D\n",
            "[INFO] 85\tBatchNormalization\n",
            "[INFO] 86\tActivation\n",
            "[INFO] 87\tConv2D\n",
            "[INFO] 88\tConv2D\n",
            "[INFO] 89\tBatchNormalization\n",
            "[INFO] 90\tBatchNormalization\n",
            "[INFO] 91\tAdd\n",
            "[INFO] 92\tActivation\n",
            "[INFO] 93\tConv2D\n",
            "[INFO] 94\tBatchNormalization\n",
            "[INFO] 95\tActivation\n",
            "[INFO] 96\tConv2D\n",
            "[INFO] 97\tBatchNormalization\n",
            "[INFO] 98\tActivation\n",
            "[INFO] 99\tConv2D\n",
            "[INFO] 100\tBatchNormalization\n",
            "[INFO] 101\tAdd\n",
            "[INFO] 102\tActivation\n",
            "[INFO] 103\tConv2D\n",
            "[INFO] 104\tBatchNormalization\n",
            "[INFO] 105\tActivation\n",
            "[INFO] 106\tConv2D\n",
            "[INFO] 107\tBatchNormalization\n",
            "[INFO] 108\tActivation\n",
            "[INFO] 109\tConv2D\n",
            "[INFO] 110\tBatchNormalization\n",
            "[INFO] 111\tAdd\n",
            "[INFO] 112\tActivation\n",
            "[INFO] 113\tConv2D\n",
            "[INFO] 114\tBatchNormalization\n",
            "[INFO] 115\tActivation\n",
            "[INFO] 116\tConv2D\n",
            "[INFO] 117\tBatchNormalization\n",
            "[INFO] 118\tActivation\n",
            "[INFO] 119\tConv2D\n",
            "[INFO] 120\tBatchNormalization\n",
            "[INFO] 121\tAdd\n",
            "[INFO] 122\tActivation\n",
            "[INFO] 123\tConv2D\n",
            "[INFO] 124\tBatchNormalization\n",
            "[INFO] 125\tActivation\n",
            "[INFO] 126\tConv2D\n",
            "[INFO] 127\tBatchNormalization\n",
            "[INFO] 128\tActivation\n",
            "[INFO] 129\tConv2D\n",
            "[INFO] 130\tBatchNormalization\n",
            "[INFO] 131\tAdd\n",
            "[INFO] 132\tActivation\n",
            "[INFO] 133\tConv2D\n",
            "[INFO] 134\tBatchNormalization\n",
            "[INFO] 135\tActivation\n",
            "[INFO] 136\tConv2D\n",
            "[INFO] 137\tBatchNormalization\n",
            "[INFO] 138\tActivation\n",
            "[INFO] 139\tConv2D\n",
            "[INFO] 140\tBatchNormalization\n",
            "[INFO] 141\tAdd\n",
            "[INFO] 142\tActivation\n",
            "[INFO] 143\tConv2D\n",
            "[INFO] 144\tBatchNormalization\n",
            "[INFO] 145\tActivation\n",
            "[INFO] 146\tConv2D\n",
            "[INFO] 147\tBatchNormalization\n",
            "[INFO] 148\tActivation\n",
            "[INFO] 149\tConv2D\n",
            "[INFO] 150\tConv2D\n",
            "[INFO] 151\tBatchNormalization\n",
            "[INFO] 152\tBatchNormalization\n",
            "[INFO] 153\tAdd\n",
            "[INFO] 154\tActivation\n",
            "[INFO] 155\tConv2D\n",
            "[INFO] 156\tBatchNormalization\n",
            "[INFO] 157\tActivation\n",
            "[INFO] 158\tConv2D\n",
            "[INFO] 159\tBatchNormalization\n",
            "[INFO] 160\tActivation\n",
            "[INFO] 161\tConv2D\n",
            "[INFO] 162\tBatchNormalization\n",
            "[INFO] 163\tAdd\n",
            "[INFO] 164\tActivation\n",
            "[INFO] 165\tConv2D\n",
            "[INFO] 166\tBatchNormalization\n",
            "[INFO] 167\tActivation\n",
            "[INFO] 168\tConv2D\n",
            "[INFO] 169\tBatchNormalization\n",
            "[INFO] 170\tActivation\n",
            "[INFO] 171\tConv2D\n",
            "[INFO] 172\tBatchNormalization\n",
            "[INFO] 173\tAdd\n",
            "[INFO] 174\tActivation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5_xGqkF6lVJ"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# a fully connect network\n",
        "class FCHeadNet:\n",
        "\t@staticmethod\n",
        "\tdef build(baseModel, classes, D):\n",
        "\t\t# initialize the head model that will be placed on top of\n",
        "\t\t# the base, then add a FC layer\n",
        "\t\theadModel = baseModel.output\n",
        "\t\theadModel = Flatten(name=\"flatten\")(headModel)\n",
        "\t\theadModel = Dense(D, activation=\"relu\")(headModel)\n",
        "\t\theadModel = Dropout(0.5)(headModel)\n",
        "\n",
        "\t\t# add a softmax layer\n",
        "\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n",
        "\n",
        "\t\t# return the model\n",
        "\t\treturn headModel"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6e9nblD6x7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3b5a87-aac7-4343-e2ca-6ac26f368d8c"
      },
      "source": [
        "# import the necessary packages\n",
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configurations related to checkpoint\n",
        "# resume or not the model\n",
        "resume = False\n",
        "\n",
        "\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                         width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,\n",
        "                         shear_range=0.15,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "# load the RGB means for the training set\n",
        "means = json.loads(open(DATASET_MEAN).read())\n",
        "\n",
        "# initialize the image preprocessors\n",
        "sp = SimplePreprocessor(224, 224)\n",
        "pp = PatchPreprocessor(224, 224)\n",
        "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=5)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=5)\n",
        "\n",
        "baseModel = ResNet50(weights=\"imagenet\", include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# initialize the new head of the network, a set of FC layers\n",
        "# followed by a softmax classifier\n",
        "num_classes = 5\n",
        "headModel = FCHeadNet.build(baseModel, num_classes, 256)\n",
        "\n",
        "# place the head FC model on top of the base model -- this will\n",
        "# become the actual model we will train\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they\n",
        "# will *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model (this needs to be done after our setting our\n",
        "# layers to being non-trainable\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "# RMSprop is frequently used in situations where we need to quickly obtain\n",
        "# reasonable performance (as is the case when we are trying to “warm up” a set of FC layers).\n",
        "opt = RMSprop(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# train the network\n",
        "history = model.fit(trainGen.generator(),\n",
        "          steps_per_epoch=trainGen.numImages // 2,\n",
        "          validation_data=valGen.generator(),\n",
        "          validation_steps=valGen.numImages // 2,\n",
        "          epochs=5,\n",
        "          max_queue_size=10,\n",
        "          verbose=1)\n",
        "\n",
        "# save the model to file\n",
        "print(\"[INFO] serializing model...\")\n",
        "model.save(MODEL_PATH, overwrite=True)\n",
        "\n",
        "# close the HDF5 datasets\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] compiling model...\n",
            "Epoch 1/5\n",
            "97/97 [==============================] - 166s 2s/step - loss: 23.2784 - accuracy: 0.6757 - val_loss: 0.7616 - val_accuracy: 0.8571\n",
            "Epoch 2/5\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.3884 - accuracy: 0.9257 - val_loss: 1.7006 - val_accuracy: 0.8286\n",
            "Epoch 3/5\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.2392 - accuracy: 0.9634 - val_loss: 1.5327 - val_accuracy: 0.9143\n",
            "Epoch 4/5\n",
            "97/97 [==============================] - 163s 2s/step - loss: 0.2132 - accuracy: 0.9740 - val_loss: 1.2736 - val_accuracy: 0.9429\n",
            "Epoch 5/5\n",
            "97/97 [==============================] - 162s 2s/step - loss: 0.1536 - accuracy: 0.9824 - val_loss: 1.9725 - val_accuracy: 0.9143\n",
            "[INFO] serializing model...\n",
            "INFO:tensorflow:Assets written to: data/cocamar.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX5TVOfzxJQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f88963e-6bb5-4c9c-b29f-40ab139f8a67"
      },
      "source": [
        "# initialize the training and validation dataset generators\n",
        "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=5)\n",
        "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=5)\n",
        "\n",
        "# now that the head FC layers have been trained/initialized, lets\n",
        "# unfreeze the final set of CONV layers and make them trainable\n",
        "for layer in baseModel.layers[165:]:\n",
        "\tlayer.trainable = True\n",
        "\n",
        "# for the changes to the model to take affect we need to recompile\n",
        "# the model, this time using SGD with a *very* small learning rate\n",
        "print(\"[INFO] re-compiling model...\")\n",
        "opt = SGD(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the model again, this time fine-tuning *both* the final set\n",
        "# of CONV layers along with our set of FC layers\n",
        "print(\"[INFO] fine-tuning model...\")\n",
        "# train the network\n",
        "history = model.fit(trainGen.generator(),\n",
        "          steps_per_epoch=trainGen.numImages // 2,\n",
        "          validation_data=valGen.generator(),\n",
        "          validation_steps=valGen.numImages // 2,\n",
        "          epochs=5,\n",
        "          max_queue_size=10,\n",
        "          verbose=1)\n",
        "\n",
        "# close the HDF5 datasets\n",
        "trainGen.close()\n",
        "valGen.close()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] re-compiling model...\n",
            "[INFO] fine-tuning model...\n",
            "Epoch 1/5\n",
            "97/97 [==============================] - 169s 2s/step - loss: 0.1100 - accuracy: 0.9878 - val_loss: 1.9690 - val_accuracy: 0.9143\n",
            "Epoch 2/5\n",
            "97/97 [==============================] - 166s 2s/step - loss: 0.0737 - accuracy: 0.9906 - val_loss: 2.0053 - val_accuracy: 0.9143\n",
            "Epoch 3/5\n",
            "97/97 [==============================] - 165s 2s/step - loss: 0.0618 - accuracy: 0.9892 - val_loss: 2.0386 - val_accuracy: 0.9143\n",
            "Epoch 4/5\n",
            "97/97 [==============================] - 166s 2s/step - loss: 0.0707 - accuracy: 0.9904 - val_loss: 2.0578 - val_accuracy: 0.9143\n",
            "Epoch 5/5\n",
            "97/97 [==============================] - 165s 2s/step - loss: 0.0554 - accuracy: 0.9913 - val_loss: 2.0387 - val_accuracy: 0.9143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSkfaOgfMd1x",
        "outputId": "b2e2edcf-094d-4665-ce97-e698a6d9ebbd"
      },
      "source": [
        "train_and_evaluate(output)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database keys ['features', 'label_names', 'labels']\n",
            "[INFO] tuning hyperparameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] best hyperparameters: {'C': 0.0001}\n",
            "[INFO] evaluating...\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "          lagarta       1.00      0.88      0.93         8\n",
            "         negative       0.80      1.00      0.89        32\n",
            " percevejo_marrom       1.00      0.76      0.87        17\n",
            "percevejo_pequeno       1.00      1.00      1.00         2\n",
            "  percevejo_verde       0.80      0.50      0.62         8\n",
            "\n",
            "         accuracy                           0.87        67\n",
            "        macro avg       0.92      0.83      0.86        67\n",
            "     weighted avg       0.88      0.87      0.86        67\n",
            "\n",
            "[INFO] saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrtfzCJ-553h"
      },
      "source": [
        "!cp -r cocamar/ /content/drive/MyDrive/Projeto/\n",
        "!cp -r data/ /content/drive/MyDrive/Projeto/"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0j_yum-BGxn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}