{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/camilabga/cocamar-bugs/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Soy Plages using Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tRBeUodgrrq"
   },
   "source": [
    "### Useful classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B47RhEPVhIxP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import BaseLogger\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import h5py\n",
    "import random\n",
    "import os\n",
    "import imutils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlHvOPCsZ5yC"
   },
   "source": [
    "### HDF5DatasetWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k-IfBT8dhDiD"
   },
   "outputs": [],
   "source": [
    "class HDF5DatasetWriter:\n",
    "  def __init__(self, dims, outputPath, dataKey=\"images\",bufSize=1000):\n",
    "    \"\"\"\n",
    "    The constructor to HDF5DatasetWriter accepts four parameters, two of which are optional.\n",
    "    \n",
    "    Args:\n",
    "    dims: controls the dimension or shape of the data we will be storing in the dataset.\n",
    "    if we were storing the (flattened) raw pixel intensities of the 28x28 = 784 MNIST dataset, \n",
    "    then dims=(70000, 784).\n",
    "    outputPath: path to where our output HDF5 file will be stored on disk.\n",
    "    datakey: The optional dataKey is the name of the dataset that will store\n",
    "    the data our algorithm will learn from.\n",
    "    bufSize: controls the size of our in-memory buffer, which we default to 1,000 feature\n",
    "    vectors/images. Once we reach bufSize, weâ€™ll flush the buffer to the HDF5 dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # check to see if the output path exists, and if so, raise\n",
    "    # an exception\n",
    "    if os.path.exists(outputPath):\n",
    "      raise ValueError(\"The supplied `outputPath` already \"\n",
    "        \"exists and cannot be overwritten. Manually delete \"\n",
    "        \"the file before continuing.\", outputPath)\n",
    "\n",
    "    # open the HDF5 database for writing and create two datasets:\n",
    "    # one to store the images/features and another to store the\n",
    "    # class labels\n",
    "    self.db = h5py.File(outputPath, \"w\")\n",
    "    self.data = self.db.create_dataset(dataKey, dims,dtype=\"float\",compression='gzip')\n",
    "    self.labels = self.db.create_dataset(\"labels\", (dims[0],),dtype=\"int\")\n",
    "\n",
    "    # store the buffer size, then initialize the buffer itself\n",
    "    # along with the index into the datasets\n",
    "    self.bufSize = bufSize\n",
    "    self.buffer = {\"data\": [], \"labels\": []}\n",
    "    self.idx = 0\n",
    "\n",
    "  def add(self, rows, labels):\n",
    "    # add the rows and labels to the buffer\n",
    "    self.buffer[\"data\"].extend(rows)\n",
    "    self.buffer[\"labels\"].extend(labels)\n",
    "\n",
    "    # check to see if the buffer needs to be flushed to disk\n",
    "    if len(self.buffer[\"data\"]) >= self.bufSize:\n",
    "      self.flush()\n",
    "\n",
    "  def flush(self):\n",
    "    # write the buffers to disk then reset the buffer\n",
    "    i = self.idx + len(self.buffer[\"data\"])\n",
    "    self.data[self.idx:i] = self.buffer[\"data\"]\n",
    "    self.labels[self.idx:i] = self.buffer[\"labels\"]\n",
    "    self.idx = i\n",
    "    self.buffer = {\"data\": [], \"labels\": []}\n",
    "\n",
    "  def storeClassLabels(self, classLabels):\n",
    "    # create a dataset to store the actual class label names,\n",
    "    # then store the class labels\n",
    "    dt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n",
    "    labelSet = self.db.create_dataset(\"label_names\",(len(classLabels),), dtype=dt)\n",
    "    labelSet[:] = classLabels\n",
    "\n",
    "  def close(self):\n",
    "    # check to see if there are any other entries in the buffer\n",
    "    # that need to be flushed to disk\n",
    "    if len(self.buffer[\"data\"]) > 0:\n",
    "      self.flush()\n",
    "\n",
    "    # close the dataset\n",
    "    self.db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGZswDVOaC3C"
   },
   "source": [
    "### Image to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fgmBX1ST1F8U"
   },
   "outputs": [],
   "source": [
    "class ImageToArrayPreprocessor:\n",
    "\tdef __init__(self, dataFormat=None):\n",
    "\t\t# store the image data format\n",
    "\t\tself.dataFormat = dataFormat\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# apply the Keras utility function that correctly rearranges\n",
    "\t\t# the dimensions of the image\n",
    "\t\treturn img_to_array(image, data_format=self.dataFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcDGt0BnaGXY"
   },
   "source": [
    "### AspectAware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7Ga-FgPH1NuW"
   },
   "outputs": [],
   "source": [
    "class AspectAwarePreprocessor:\n",
    "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
    "\t\t# store the target image width, height, and interpolation\n",
    "\t\t# method used when resizing\n",
    "\t\tself.width = width\n",
    "\t\tself.height = height\n",
    "\t\tself.inter = inter\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# grab the dimensions of the image and then initialize\n",
    "\t\t# the deltas to use when cropping\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\tdW = 0\n",
    "\t\tdH = 0\n",
    "\n",
    "\t\t# if the width is smaller than the height, then resize\n",
    "\t\t# along the width (i.e., the smaller dimension) and then\n",
    "\t\t# update the deltas to crop the height to the desired\n",
    "\t\t# dimension\n",
    "\t\tif w < h:\n",
    "\t\t\timage = imutils.resize(image, width=self.width,\n",
    "\t\t\t\tinter=self.inter)\n",
    "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
    "\n",
    "\t\t# otherwise, the height is smaller than the width so\n",
    "\t\t# resize along the height and then update the deltas\n",
    "\t\t# crop along the width\n",
    "\t\telse:\n",
    "\t\t\timage = imutils.resize(image, height=self.height,\n",
    "\t\t\t\tinter=self.inter)\n",
    "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
    "\n",
    "\t\t# now that our images have been resized, we need to\n",
    "\t\t# re-grab the width and height, followed by performing\n",
    "\t\t# the crop\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\timage = image[dH:h - dH, dW:w - dW]\n",
    "\n",
    "\t\t# finally, resize the image to the provided spatial\n",
    "\t\t# dimensions to ensure our output image is always a fixed\n",
    "\t\t# size\n",
    "\t\treturn cv2.resize(image, (self.width, self.height),\n",
    "\t\t\tinterpolation=self.inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XgdXcA7aPfT"
   },
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mfbg-dCsUEuW"
   },
   "outputs": [],
   "source": [
    "class MeanPreprocessor:\n",
    "\tdef __init__(self, rMean, gMean, bMean):\n",
    "\t\t# store the Red, Green, and Blue channel averages across a\n",
    "\t\t# training set\n",
    "\t\tself.rMean = rMean\n",
    "\t\tself.gMean = gMean\n",
    "\t\tself.bMean = bMean\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# split the image into its respective Red, Green, and Blue\n",
    "\t\t# channels\n",
    "\t\t(B, G, R) = cv2.split(image.astype(\"float32\"))\n",
    "\n",
    "\t\t# subtract the means for each channel\n",
    "\t\tR -= self.rMean\n",
    "\t\tG -= self.gMean\n",
    "\t\tB -= self.bMean\n",
    "\n",
    "    # Keep in mind that OpenCV represents images in BGR order\n",
    "\t\t# merge the channels back together and return the image\n",
    "\t\treturn cv2.merge([B, G, R])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COixYybsaWyp"
   },
   "source": [
    "### Image Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bm2bRAs2UKHj"
   },
   "outputs": [],
   "source": [
    "class PatchPreprocessor:\n",
    "\tdef __init__(self, width, height):\n",
    "\t\t# store the target width and height of the image\n",
    "\t\tself.width = width\n",
    "\t\tself.height = height\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# extract a random crop from the image with the target width\n",
    "\t\t# and height\n",
    "\t\treturn extract_patches_2d(image, (self.height, self.width),\n",
    "\t\t\tmax_patches=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gft-PE3aJvZ"
   },
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jc0kftNdR2lk"
   },
   "outputs": [],
   "source": [
    "class CropPreprocessor:\n",
    "\tdef __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA):\n",
    "\t\t# store the target image width, height, whether or not\n",
    "\t\t# horizontal flips should be included, along with the\n",
    "\t\t# interpolation method used when resizing\n",
    "\t\tself.width = width\n",
    "\t\tself.height = height\n",
    "\t\tself.horiz = horiz\n",
    "\t\tself.inter = inter\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# initialize the list of crops\n",
    "\t\tcrops = []\n",
    "\n",
    "\t\t# grab the width and height of the image then use these\n",
    "\t\t# dimensions to define the corners of the image based\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\tcoords = [\n",
    "\t\t\t[0, 0, self.width, self.height],\n",
    "\t\t\t[w - self.width, 0, w, self.height],\n",
    "\t\t\t[w - self.width, h - self.height, w, h],\n",
    "\t\t\t[0, h - self.height, self.width, h]]\n",
    "\n",
    "\t\t# compute the center crop of the image as well\n",
    "\t\tdW = int(0.5 * (w - self.width))\n",
    "\t\tdH = int(0.5 * (h - self.height))\n",
    "\t\tcoords.append([dW, dH, w - dW, h - dH])\n",
    "\n",
    "\t\t# loop over the coordinates, extract each of the crops,\n",
    "\t\t# and resize each of them to a fixed size\n",
    "\t\tfor (startX, startY, endX, endY) in coords:\n",
    "\t\t\tcrop = image[startY:endY, startX:endX]\n",
    "\t\t\tcrop = cv2.resize(crop, (self.width, self.height),\n",
    "\t\t\t\tinterpolation=self.inter)\n",
    "\t\t\tcrops.append(crop)\n",
    "\n",
    "\t\t# check to see if the horizontal flips should be taken\n",
    "\t\tif self.horiz:\n",
    "\t\t\t# compute the horizontal mirror flips for each crop\n",
    "\t\t\tmirrors = [cv2.flip(c, 1) for c in crops]\n",
    "\t\t\tcrops.extend(mirrors)\n",
    "\n",
    "\t\t# return the set of crops\n",
    "\t\treturn np.array(crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NF8faSog8so"
   },
   "source": [
    "### HDF5 dataset generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uMo22QKUg7me"
   },
   "outputs": [],
   "source": [
    "class HDF5DatasetGenerator:\n",
    "\tdef __init__(self, dbPath, batchSize, preprocessors=None, aug=None, binarize=True, classes=2):\n",
    "\t\t# store the batch size, preprocessors, and data augmentor,\n",
    "\t\t# whether or not the labels should be binarized, along with\n",
    "\t\t# the total number of classes\n",
    "\t\tself.batchSize = batchSize\n",
    "\t\tself.preprocessors = preprocessors\n",
    "\t\tself.aug = aug\n",
    "\t\tself.binarize = binarize\n",
    "\t\tself.classes = classes\n",
    "\n",
    "\t\t# open the HDF5 database for reading and determine the total\n",
    "\t\t# number of entries in the database\n",
    "\t\tself.db = h5py.File(dbPath, \"r\")\n",
    "\t\tself.numImages = self.db[\"labels\"].shape[0]\n",
    "\n",
    "\tdef generator(self, passes=np.inf):\n",
    "\t\t# initialize the epoch count\n",
    "\t\tepochs = 0\n",
    "\n",
    "\t\t# keep looping infinitely -- the model will stop once we have\n",
    "\t\t# reach the desired number of epochs\n",
    "\t\twhile epochs < passes:\n",
    "\t\t\t# loop over the HDF5 dataset\n",
    "\t\t\tfor i in np.arange(0, self.numImages, self.batchSize):\n",
    "\t\t\t\t# extract the images and labels from the HDF dataset\n",
    "\t\t\t\timages = self.db[\"images\"][i: i + self.batchSize]\n",
    "\t\t\t\tlabels = self.db[\"labels\"][i: i + self.batchSize]\n",
    "\n",
    "\t\t\t\t# check to see if the labels should be binarized\n",
    "\t\t\t\tif self.binarize:\n",
    "\t\t\t\t\tlabels = to_categorical(labels,\n",
    "\t\t\t\t\t\tself.classes)\n",
    "\n",
    "\t\t\t\t# check to see if our preprocessors are not None\n",
    "\t\t\t\tif self.preprocessors is not None:\n",
    "\t\t\t\t\t# initialize the list of processed images\n",
    "\t\t\t\t\tprocImages = []\n",
    "\n",
    "\t\t\t\t\t# loop over the images\n",
    "\t\t\t\t\tfor image in images:\n",
    "\t\t\t\t\t\t# loop over the preprocessors and apply each\n",
    "\t\t\t\t\t\t# to the image\n",
    "\t\t\t\t\t\tfor p in self.preprocessors:\n",
    "\t\t\t\t\t\t\timage = p.preprocess(image)\n",
    "\n",
    "\t\t\t\t\t\t# update the list of processed images\n",
    "\t\t\t\t\t\tprocImages.append(image)\n",
    "\n",
    "\t\t\t\t\t# update the images array to be the processed\n",
    "\t\t\t\t\t# images\n",
    "\t\t\t\t\timages = np.array(procImages)\n",
    "\n",
    "\t\t\t\t# if the data augmenator exists, apply it\n",
    "\t\t\t\tif self.aug is not None:\n",
    "\t\t\t\t\t(images, labels) = next(self.aug.flow(images,\n",
    "\t\t\t\t\t\tlabels, batch_size=self.batchSize))\n",
    "\n",
    "\t\t\t\t# yield a tuple of images and labels\n",
    "\t\t\t\tyield (images, labels)\n",
    "\n",
    "\t\t\t# increment the total number of epochs\n",
    "\t\t\tepochs += 1\n",
    "\n",
    "\tdef close(self):\n",
    "\t\t# close the database\n",
    "\t\tself.db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ou-_XnZqjrY"
   },
   "source": [
    "### Simple preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Juf1YRHzqmeU"
   },
   "outputs": [],
   "source": [
    "class SimplePreprocessor:\n",
    "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
    "\t\t# store the target image width, height, and interpolation\n",
    "\t\t# method used when resizing\n",
    "\t\tself.width = width\n",
    "\t\tself.height = height\n",
    "\t\tself.inter = inter\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# resize the image to a fixed size, ignoring the aspect\n",
    "\t\t# ratio\n",
    "\t\treturn cv2.resize(image, (self.width, self.height),\n",
    "\t\t\tinterpolation=self.inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bJUx9Prq2xE"
   },
   "source": [
    "### Training monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "m1JGywYeq5Wj"
   },
   "outputs": [],
   "source": [
    "class TrainingMonitor(BaseLogger):\n",
    "\tdef __init__(self, figPath, jsonPath=None, startAt=0):\n",
    "\t\t# store the output path for the figure, the path to the JSON\n",
    "\t\t# serialized file, and the starting epoch\n",
    "\t\tsuper(TrainingMonitor, self).__init__()\n",
    "\t\tself.figPath = figPath\n",
    "\t\tself.jsonPath = jsonPath\n",
    "\t\tself.startAt = startAt\n",
    "\n",
    "\tdef on_train_begin(self, logs={}):\n",
    "\t\t# initialize the history dictionary\n",
    "\t\tself.H = {}\n",
    "\n",
    "\t\t# if the JSON history path exists, load the training history\n",
    "\t\tif self.jsonPath is not None:\n",
    "\t\t\tif os.path.exists(self.jsonPath):\n",
    "\t\t\t\tself.H = json.loads(open(self.jsonPath).read())\n",
    "\n",
    "\t\t\t\t# check to see if a starting epoch was supplied\n",
    "\t\t\t\tif self.startAt > 0:\n",
    "\t\t\t\t\t# loop over the entries in the history log and\n",
    "\t\t\t\t\t# trim any entries that are past the starting\n",
    "\t\t\t\t\t# epoch\n",
    "\t\t\t\t\tfor k in self.H.keys():\n",
    "\t\t\t\t\t\tself.H[k] = self.H[k][:self.startAt]\n",
    "\n",
    "\tdef on_epoch_end(self, epoch, logs={}):\n",
    "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
    "\t\t# for the entire training process\n",
    "\t\tfor (k, v) in logs.items():\n",
    "\t\t\tl = self.H.get(k, [])\n",
    "\t\t\tl.append(float(v))\n",
    "\t\t\tself.H[k] = l\n",
    "\n",
    "\t\t# check to see if the training history should be serialized\n",
    "\t\t# to file\n",
    "\t\tif self.jsonPath is not None:\n",
    "\t\t\tf = open(self.jsonPath, \"w\")\n",
    "\t\t\tf.write(json.dumps(self.H))\n",
    "\t\t\tf.close()\n",
    "\n",
    "\t\t# ensure at least two epochs have passed before plotting\n",
    "\t\t# (epoch starts at zero)\n",
    "\t\tif len(self.H[\"loss\"]) > 1:\n",
    "\t\t\t# plot the training loss and accuracy\n",
    "\t\t\tN = np.arange(0, len(self.H[\"loss\"]))\n",
    "\t\t\tplt.style.use(\"ggplot\")\n",
    "\t\t\tplt.figure()\n",
    "\t\t\tplt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n",
    "\t\t\tplt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n",
    "\t\t\tplt.plot(N, self.H[\"accuracy\"], label=\"train_acc\")\n",
    "\t\t\tplt.plot(N, self.H[\"val_accuracy\"], label=\"val_acc\")\n",
    "\t\t\tplt.title(\"Training Loss and Accuracy [Epoch {}]\".format(\n",
    "\t\t\t\tlen(self.H[\"loss\"])))\n",
    "\t\t\tplt.xlabel(\"Epoch #\")\n",
    "\t\t\tplt.ylabel(\"Loss/Accuracy\")\n",
    "\t\t\tplt.legend()\n",
    "\n",
    "\t\t\t# save the figure\n",
    "\t\t\tplt.savefig(self.figPath)\n",
    "\t\t\tplt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk2HBr6JAMQ8"
   },
   "source": [
    "### Rank accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AJCbMd1sAPP8"
   },
   "outputs": [],
   "source": [
    "def rank5_accuracy(preds, labels):\n",
    "\t# initialize the rank-1 and rank-5 accuracies\n",
    "\trank1 = 0\n",
    "\trank5 = 0\n",
    "\n",
    "\t# loop over the predictions and ground-truth labels\n",
    "\tfor (p, gt) in zip(preds, labels):\n",
    "\t\t# sort the probabilities by their index in descending\n",
    "\t\t# order so that the more confident guesses are at the\n",
    "\t\t# front of the list\n",
    "\t\tp = np.argsort(p)[::-1]\n",
    "\n",
    "\t\t# check if the ground-truth label is in the top-5\n",
    "\t\t# predictions\n",
    "\t\tif gt in p[:5]:\n",
    "\t\t\trank5 += 1\n",
    "\n",
    "\t\t# check to see if the ground-truth is the #1 prediction\n",
    "\t\tif gt == p[0]:\n",
    "\t\t\trank1 += 1\n",
    "\n",
    "\t# compute the final rank-1 and rank-5 accuracies\n",
    "\trank1 /= float(len(preds))\n",
    "\trank5 /= float(len(preds))\n",
    "\n",
    "\t# return a tuple of the rank-1 and rank-5 accuracies\n",
    "\treturn (rank1, rank5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrmCQKbMHI-4"
   },
   "source": [
    "### SimpleDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wj4RbTs8HH62"
   },
   "outputs": [],
   "source": [
    "class SimpleDatasetLoader:\n",
    "\tdef __init__(self, preprocessors=None):\n",
    "\t\t# store the image preprocessor\n",
    "\t\tself.preprocessors = preprocessors\n",
    "\n",
    "\t\t# if the preprocessors are None, initialize them as an\n",
    "\t\t# empty list\n",
    "\t\tif self.preprocessors is None:\n",
    "\t\t\tself.preprocessors = []\n",
    "\n",
    "\tdef load(self, imagePaths, verbose=-1):\n",
    "\t\t# initialize the list of features and labels\n",
    "\t\tdata = []\n",
    "\t\tlabels = []\n",
    "\n",
    "\t\t# loop over the input images\n",
    "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
    "\t\t\t# load the image and extract the class label assuming\n",
    "\t\t\t# that our path has the following format:\n",
    "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
    "\t\t\timage = cv2.imread(imagePath)\n",
    "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "\t\t\t# check to see if our preprocessors are not None\n",
    "\t\t\tif self.preprocessors is not None:\n",
    "\t\t\t\t# loop over the preprocessors and apply each to\n",
    "\t\t\t\t# the image\n",
    "\t\t\t\tfor p in self.preprocessors:\n",
    "\t\t\t\t\timage = p.preprocess(image)\n",
    "\n",
    "\t\t\t# treat our processed image as a \"feature vector\"\n",
    "\t\t\t# by updating the data list followed by the labels\n",
    "\t\t\tdata.append(image)\n",
    "\t\t\tlabels.append(label)\n",
    "\n",
    "\t\t\t# show an update every `verbose` images\n",
    "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
    "\t\t\t\t\tlen(imagePaths)))\n",
    "\n",
    "\t\t# return a tuple of the data and labels\n",
    "\t\treturn (np.array(data), np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwDVpznk2rZ0"
   },
   "source": [
    "### Working with HDFS and large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "K9e4A2OIfmDO",
    "outputId": "1f256a8d-8ed6-45b9-ed1b-658c064f74ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "   creating: data/train/\n",
      "  inflating: data/train/lagarta.5.jpg  \n",
      "  inflating: data/train/lagarta.6.jpg  \n",
      "  inflating: data/train/lagarta.7.jpg  \n",
      "  inflating: data/train/lagarta.8.jpg  \n",
      "  inflating: data/train/lagarta.9.jpg  \n",
      "  inflating: data/train/lagarta.10.jpg  \n",
      "  inflating: data/train/lagarta.13.jpg  \n",
      "  inflating: data/train/lagarta.14.jpg  \n",
      "  inflating: data/train/lagarta.1.jpg  \n",
      "  inflating: data/train/lagarta.2.jpg  \n",
      "  inflating: data/train/lagarta.11.jpg  \n",
      "  inflating: data/train/lagarta.12.jpg  \n",
      "  inflating: data/train/lagarta.15.jpg  \n",
      "  inflating: data/train/lagarta.3.png  \n",
      "  inflating: data/train/lagarta.4.jpg  \n",
      "  inflating: data/train/lagarta.17.jpg  \n",
      "  inflating: data/train/lagarta.16.jpg  \n",
      "  inflating: data/train/lagarta.18.jfif  \n",
      "  inflating: data/train/lagarta.19.jfif  \n",
      "  inflating: data/train/lagarta.20.jpg  \n",
      "  inflating: data/train/lagarta.21.jpg  \n",
      "  inflating: data/train/lagarta.22.png  \n",
      "  inflating: data/train/lagarta.23.png  \n",
      "  inflating: data/train/lagarta.27.png  \n",
      "  inflating: data/train/lagarta.26.JPG  \n",
      "  inflating: data/train/lagarta.25.jpeg  \n",
      "  inflating: data/train/lagarta.28.jpg  \n",
      "  inflating: data/train/lagarta.29.jfif  \n",
      "  inflating: data/train/lagarta.24.jfif  \n",
      "  inflating: data/train/negative.1.jpg  \n",
      "  inflating: data/train/negative.2.jpg  \n",
      "  inflating: data/train/negative.3.jpg  \n",
      "  inflating: data/train/negative.4.jpg  \n",
      "  inflating: data/train/negative.5.jpg  \n",
      "  inflating: data/train/negative.6.jpg  \n",
      "  inflating: data/train/negative.7.jpg  \n",
      "  inflating: data/train/negative.8.jpg  \n",
      "  inflating: data/train/negative.9.jpg  \n",
      "  inflating: data/train/negative.10.jpg  \n",
      "  inflating: data/train/negative.11.jpg  \n",
      "  inflating: data/train/negative.12.jpg  \n",
      "  inflating: data/train/negative.13.jpg  \n",
      "  inflating: data/train/negative.14.jpg  \n",
      "  inflating: data/train/negative.15.jpg  \n",
      "  inflating: data/train/negative.16.jpg  \n",
      "  inflating: data/train/negative.17.jpg  \n",
      "  inflating: data/train/negative.18.jpg  \n",
      "  inflating: data/train/negative.19.jpg  \n",
      "  inflating: data/train/negative.20.jpg  \n",
      "  inflating: data/train/negative.21.jpg  \n",
      "  inflating: data/train/negative.22.jpg  \n",
      "  inflating: data/train/negative.23.jpg  \n",
      "  inflating: data/train/negative.24.jpg  \n",
      "  inflating: data/train/negative.25.jpg  \n",
      "  inflating: data/train/negative.26.jpg  \n",
      "  inflating: data/train/negative.27.jpg  \n",
      "  inflating: data/train/negative.28.jpg  \n",
      "  inflating: data/train/negative.29.jpg  \n",
      "  inflating: data/train/negative.30.jpg  \n",
      "  inflating: data/train/negative.31.jpg  \n",
      "  inflating: data/train/negative.32.jpg  \n",
      "  inflating: data/train/negative.34.jpg  \n",
      "  inflating: data/train/negative.35.jpg  \n",
      "  inflating: data/train/negative.36.jpg  \n",
      "  inflating: data/train/negative.37.png  \n",
      "  inflating: data/train/negative.38.jpg  \n",
      "  inflating: data/train/negative.39.jpg  \n",
      "  inflating: data/train/negative.40.jpg  \n",
      "  inflating: data/train/negative.41.jpg  \n",
      "  inflating: data/train/negative.42.jpg  \n",
      "  inflating: data/train/negative.43.jpg  \n",
      "  inflating: data/train/negative.44.jpg  \n",
      "  inflating: data/train/negative.45.jpg  \n",
      "  inflating: data/train/negative.46.jpg  \n",
      "  inflating: data/train/negative.47.jpg  \n",
      "  inflating: data/train/negative.48.jpg  \n",
      "  inflating: data/train/negative.49.jpg  \n",
      "  inflating: data/train/negative.50.jpg  \n",
      "  inflating: data/train/negative.51.jpg  \n",
      "  inflating: data/train/negative.52.jpg  \n",
      "  inflating: data/train/negative.53.jpg  \n",
      "  inflating: data/train/negative.54.jpg  \n",
      "  inflating: data/train/negative.55.jpg  \n",
      "  inflating: data/train/negative.56.jpg  \n",
      "  inflating: data/train/negative.57.jpg  \n",
      "  inflating: data/train/negative.58.jpg  \n",
      "  inflating: data/train/negative.59.jpg  \n",
      "  inflating: data/train/negative.60.jpg  \n",
      "  inflating: data/train/negative.61.jpg  \n",
      "  inflating: data/train/negative.62.png  \n",
      "  inflating: data/train/negative.63.png  \n",
      "  inflating: data/train/negative.64.jpg  \n",
      "  inflating: data/train/negative.65.jpg  \n",
      "  inflating: data/train/negative.74.jpg  \n",
      "  inflating: data/train/negative.75.jpg  \n",
      "  inflating: data/train/negative.76.jpg  \n",
      "  inflating: data/train/negative.77.jpg  \n",
      "  inflating: data/train/negative.78.jpg  \n",
      "  inflating: data/train/negative.79.jpg  \n",
      "  inflating: data/train/negative.80.jpg  \n",
      "  inflating: data/train/negative.81.jpg  \n",
      "  inflating: data/train/negative.82.jpg  \n",
      "  inflating: data/train/negative.66.jpg  \n",
      "  inflating: data/train/negative.83.jpg  \n",
      "  inflating: data/train/negative.84.jpg  \n",
      "  inflating: data/train/negative.85.jpg  \n",
      "  inflating: data/train/negative.86.jpg  \n",
      "  inflating: data/train/negative.87.jpg  \n",
      "  inflating: data/train/negative.88.jpg  \n",
      "  inflating: data/train/negative.89.jpg  \n",
      "  inflating: data/train/negative.90.jpg  \n",
      "  inflating: data/train/negative.91.jpg  \n",
      "  inflating: data/train/negative.92.jpg  \n",
      "  inflating: data/train/negative.67.jpg  \n",
      "  inflating: data/train/negative.93.jpg  \n",
      "  inflating: data/train/negative.94.jpg  \n",
      "  inflating: data/train/negative.95.jpg  \n",
      "  inflating: data/train/negative.96.jpg  \n",
      "  inflating: data/train/negative.97.jpg  \n",
      "  inflating: data/train/negative.98.jpg  \n",
      "  inflating: data/train/negative.99.jpg  \n",
      "  inflating: data/train/negative.100.jpg  \n",
      "  inflating: data/train/negative.101.jpg  \n",
      "  inflating: data/train/negative.68.jpg  \n",
      "  inflating: data/train/negative.102.jpg  \n",
      "  inflating: data/train/negative.103.jpg  \n",
      "  inflating: data/train/negative.104.jpg  \n",
      "  inflating: data/train/negative.105.jpg  \n",
      "  inflating: data/train/negative.106.jpg  \n",
      "  inflating: data/train/negative.107.jpg  \n",
      "  inflating: data/train/negative.108.png  \n",
      "  inflating: data/train/negative.109.jpg  \n",
      "  inflating: data/train/negative.69.jpg  \n",
      "  inflating: data/train/negative.70.jpg  \n",
      "  inflating: data/train/negative.71.jpg  \n",
      "  inflating: data/train/negative.72.jpg  \n",
      "  inflating: data/train/negative.73.jpg  \n",
      "  inflating: data/train/negative.110.jpg  \n",
      "  inflating: data/train/negative.111.jpg  \n",
      "  inflating: data/train/negative.112.jpg  \n",
      "  inflating: data/train/negative.113.jpg  \n",
      "  inflating: data/train/negative.114.jpeg  \n",
      "  inflating: data/train/negative.115.jpg  \n",
      "  inflating: data/train/negative.116.jpg  \n",
      "  inflating: data/train/negative.117.jpg  \n",
      "  inflating: data/train/negative.118.jpg  \n",
      "  inflating: data/train/negative.119.jpg  \n",
      "  inflating: data/train/negative.120.jpg  \n",
      "  inflating: data/train/negative.121.jpg  \n",
      "  inflating: data/train/negative.122.jpg  \n",
      "  inflating: data/train/negative.123.jpg  \n",
      "  inflating: data/train/negative.124.jpg  \n",
      "  inflating: data/train/negative.125.jpg  \n",
      "  inflating: data/train/negative.33.png  \n",
      "  inflating: data/train/percevejo_marrom.3.jpg  \n",
      "  inflating: data/train/percevejo_marrom.15.jpg  \n",
      "  inflating: data/train/percevejo_marrom.6.jpg  \n",
      "  inflating: data/train/percevejo_marrom.7.jpg  \n",
      "  inflating: data/train/percevejo_marrom.8.jpg  \n",
      "  inflating: data/train/percevejo_marrom.9.jpg  \n",
      "  inflating: data/train/percevejo_marrom.10.jpg  \n",
      "  inflating: data/train/percevejo_marrom.22.jpg  \n",
      "  inflating: data/train/percevejo_marrom.23.jpg  \n",
      "  inflating: data/train/percevejo_marrom.24.jpg  \n",
      "  inflating: data/train/percevejo_marrom.16.jpg  \n",
      "  inflating: data/train/percevejo_marrom.25.jpg  \n",
      "  inflating: data/train/percevejo_marrom.17.jpg  \n",
      "  inflating: data/train/percevejo_marrom.11.jpg  \n",
      "  inflating: data/train/percevejo_marrom.12.jpg  \n",
      "  inflating: data/train/percevejo_marrom.18.jpg  \n",
      "  inflating: data/train/percevejo_marrom.4.jpg  \n",
      "  inflating: data/train/percevejo_marrom.26.jpg  \n",
      "  inflating: data/train/percevejo_marrom.19.jpg  \n",
      "  inflating: data/train/percevejo_marrom.1.jpg  \n",
      "  inflating: data/train/percevejo_marrom.20.jpg  \n",
      "  inflating: data/train/percevejo_marrom.13.jpg  \n",
      "  inflating: data/train/percevejo_marrom.14.jpg  \n",
      "  inflating: data/train/percevejo_marrom.2.jpg  \n",
      "  inflating: data/train/percevejo_marrom.21.jpg  \n",
      "  inflating: data/train/percevejo_marrom.5.jpg  \n",
      "  inflating: data/train/percevejo_marrom.27.jpg  \n",
      "  inflating: data/train/percevejo_marrom.28.jpg  \n",
      "  inflating: data/train/percevejo_marrom.29.jpg  \n",
      "  inflating: data/train/percevejo_marrom.30.jpg  \n",
      "  inflating: data/train/percevejo_marrom.31.jpg  \n",
      "  inflating: data/train/percevejo_marrom.33.jpg  \n",
      "  inflating: data/train/percevejo_marrom.32.jpg  \n",
      "  inflating: data/train/percevejo_marrom.34.jpg  \n",
      "  inflating: data/train/percevejo_marrom.35.png  \n",
      "  inflating: data/train/percevejo_marrom.36.JPG  \n",
      "  inflating: data/train/percevejo_marrom.39.jpg  \n",
      "  inflating: data/train/percevejo_marrom.37.jpg  \n",
      "  inflating: data/train/percevejo_marrom.38.jpg  \n",
      "  inflating: data/train/percevejo_marrom.40.jpg  \n",
      "  inflating: data/train/percevejo_marrom.41.png  \n",
      "  inflating: data/train/percevejo_marrom.42.jpg  \n",
      "  inflating: data/train/percevejo_marrom.43.jfif  \n",
      "  inflating: data/train/percevejo_marrom.44.jpg  \n",
      "  inflating: data/train/percevejo_marrom.45.jpg  \n",
      "  inflating: data/train/percevejo_marrom.47.png  \n",
      "  inflating: data/train/percevejo_marrom.46.png  \n",
      "  inflating: data/train/percevejo_marrom.48.jpg  \n",
      "  inflating: data/train/percevejo_marrom.49.jpg  \n",
      "  inflating: data/train/percevejo_marrom.50.jpg  \n",
      "  inflating: data/train/percevejo_marrom.51.jpg  \n",
      "  inflating: data/train/percevejo_marrom.52.jpg  \n",
      "  inflating: data/train/percevejo_marrom.53.jpg  \n",
      "  inflating: data/train/percevejo_marrom.54.jpg  \n",
      "  inflating: data/train/percevejo_marrom.55.jpg  \n",
      "  inflating: data/train/percevejo_marrom.56.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.1.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.2.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.3.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.4.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.5.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.6.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.7.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.8.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.9.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.21.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.10.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.11.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.12.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.13.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.14.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.15.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.16.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.17.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.18.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.19.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.20.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.22.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.23.jpg  \n",
      "  inflating: data/train/percevejo_pequeno.24.jpg  \n",
      "  inflating: data/train/percevejo_verde.1.jpg  \n",
      "  inflating: data/train/percevejo_verde.2.jpg  \n",
      "  inflating: data/train/percevejo_verde.3.jpg  \n",
      "  inflating: data/train/percevejo_verde.4.jpg  \n",
      "  inflating: data/train/percevejo_verde.5.jpg  \n",
      "  inflating: data/train/percevejo_verde.20.jpg  \n",
      "  inflating: data/train/percevejo_verde.6.jpg  \n",
      "  inflating: data/train/percevejo_verde.30.png  \n",
      "  inflating: data/train/percevejo_verde.7.jpg  \n",
      "  inflating: data/train/percevejo_verde.21.jpg  \n",
      "  inflating: data/train/percevejo_verde.22.jpg  \n",
      "  inflating: data/train/percevejo_verde.23.jpg  \n",
      "  inflating: data/train/percevejo_verde.24.jpg  \n",
      "  inflating: data/train/percevejo_verde.31.jpg  \n",
      "  inflating: data/train/percevejo_verde.25.jpg  \n",
      "  inflating: data/train/percevejo_verde.26.jpg  \n",
      "  inflating: data/train/percevejo_verde.27.jpg  \n",
      "  inflating: data/train/percevejo_verde.28.jpg  \n",
      "  inflating: data/train/percevejo_verde.8.jpg  \n",
      "  inflating: data/train/percevejo_verde.9.jpg  \n",
      "  inflating: data/train/percevejo_verde.10.jpg  \n",
      "  inflating: data/train/percevejo_verde.11.jpg  \n",
      "  inflating: data/train/percevejo_verde.12.jpg  \n",
      "  inflating: data/train/percevejo_verde.13.jpg  \n",
      "  inflating: data/train/percevejo_verde.14.jpg  \n",
      "  inflating: data/train/percevejo_verde.15.jpg  \n",
      "  inflating: data/train/percevejo_verde.16.jpg  \n",
      "  inflating: data/train/percevejo_verde.17.jpg  \n",
      "  inflating: data/train/percevejo_verde.18.jpg  \n",
      "  inflating: data/train/percevejo_verde.19.jpg  \n",
      "  inflating: data/train/percevejo_verde.29.jpg  \n",
      "  inflating: data/train/percevejo_verde.32.jpg  \n",
      "  inflating: data/train/percevejo_verde.33.jpg  \n",
      "  inflating: data/train/percevejo_verde.34.png  \n",
      "  inflating: data/train/percevejo_verde.35.png  \n",
      "  inflating: data/train/percevejo_verde.36.jpg  \n"
     ]
    }
   ],
   "source": [
    "!unzip \"data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LCmYyH7NoDcS"
   },
   "outputs": [],
   "source": [
    "!mkdir data/hdf5\n",
    "!mkdir data/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KQjEjj79tCV5",
    "outputId": "75f2f687-c688-48d4-ba2e-bfc9915b20ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    270     270    5195\n"
     ]
    }
   ],
   "source": [
    "ls data/train | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Jl7hYvPJo4y_"
   },
   "outputs": [],
   "source": [
    "# define the paths to the images directory\n",
    "IMAGES_PATH = \"data/train\"\n",
    "\n",
    "# since we do not have validation data or access to the testing\n",
    "# labels we need to take a number of images from the training\n",
    "# data and use them instead\n",
    "NUM_CLASSES = 5\n",
    "NUM_VAL_IMAGES = 7 * NUM_CLASSES\n",
    "NUM_TEST_IMAGES = 7 * NUM_CLASSES\n",
    "\n",
    "# define the path to the output training, validation, and testing\n",
    "# HDF5 files\n",
    "TRAIN_HDF5 = \"data/hdf5/train.hdf5\"\n",
    "VAL_HDF5 = \"data/hdf5/val.hdf5\"\n",
    "TEST_HDF5 = \"data/hdf5/test.hdf5\"\n",
    "\n",
    "# path to the output model file\n",
    "MODEL_PATH = \"data/cocamar.model\"\n",
    "\n",
    "# define the path to the dataset mean\n",
    "DATASET_MEAN = \"data/cocamar.json\"\n",
    "\n",
    "# define the path to the output directory used for storing plots,\n",
    "# classification reports, etc.\n",
    "OUTPUT_PATH = \"data/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Omp0jxsm9Yr9",
    "outputId": "ad81c4d8-ab5b-4923-c200-eb402fdfdb25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Dataset:   2% |#                                      | ETA:   0:00:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building data/hdf5/train.hdf5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Dataset: 100% |#######################################| Time:  0:00:01\n",
      "Building Dataset:  51% |####################                   | ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building data/hdf5/val.hdf5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Dataset: 100% |#######################################| Time:  0:00:00\n",
      "Building Dataset:  60% |#######################                | ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building data/hdf5/test.hdf5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Dataset: 100% |#######################################| Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing means...\n"
     ]
    }
   ],
   "source": [
    "# grab the paths to the images\n",
    "trainPaths = list(paths.list_images(IMAGES_PATH))\n",
    "trainLabels = [p.split(os.path.sep)[-1].split(\".\")[0] for p in trainPaths]\n",
    "le = LabelEncoder()\n",
    "trainLabels = le.fit_transform(trainLabels)\n",
    "\n",
    "# perform stratified sampling from the training set to build the\n",
    "# testing split from the training data\n",
    "split = train_test_split(trainPaths, trainLabels,\n",
    "                         test_size=NUM_TEST_IMAGES, \n",
    "                         stratify=trainLabels,random_state=42)\n",
    "(trainPaths, testPaths, trainLabels, testLabels) = split\n",
    "\n",
    "# perform another stratified sampling, this time to build the\n",
    "# validation data\n",
    "split = train_test_split(trainPaths, trainLabels,\n",
    "                         test_size=NUM_VAL_IMAGES, \n",
    "                         stratify=trainLabels,random_state=42)\n",
    "(trainPaths, valPaths, trainLabels, valLabels) = split\n",
    "\n",
    "# construct a list pairing the training, validation, and testing\n",
    "# image paths along with their corresponding labels and output HDF5\n",
    "# files\n",
    "datasets = [\n",
    "\t(\"train\", trainPaths, trainLabels, TRAIN_HDF5),\n",
    "\t(\"val\", valPaths, valLabels, VAL_HDF5),\n",
    "\t(\"test\", testPaths, testLabels, TEST_HDF5)]\n",
    "\n",
    "# initialize the image pre-processor and the lists of RGB channel\n",
    "# averages\n",
    "aap = AspectAwarePreprocessor(256, 256)\n",
    "(R, G, B) = ([], [], [])\n",
    "\n",
    "# loop over the dataset tuples\n",
    "for (dType, paths, labels, outputPath) in datasets:\n",
    "\t# create HDF5 writer\n",
    "\tprint(\"[INFO] building {}...\".format(outputPath))\n",
    "\twriter = HDF5DatasetWriter((len(paths), 256, 256, 3), outputPath)\n",
    "\n",
    "\t# initialize the progress bar\n",
    "\twidgets = [\"Building Dataset: \", progressbar.Percentage(), \" \",progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "\tpbar = progressbar.ProgressBar(maxval=len(paths),widgets=widgets).start()\n",
    "\n",
    "\t# loop over the image paths\n",
    "\tfor (i, (path, label)) in enumerate(zip(paths, labels)):\n",
    "\t\t# load the image and process it\n",
    "\t\timage = cv2.imread(path)\n",
    "\t\timage = aap.preprocess(image)\n",
    "\n",
    "\t\t# if we are building the training dataset, then compute the\n",
    "\t\t# mean of each channel in the image, then update the\n",
    "\t\t# respective lists\n",
    "\t\tif dType == \"train\":\n",
    "\t\t\t(b, g, r) = cv2.mean(image)[:3]\n",
    "\t\t\tR.append(r)\n",
    "\t\t\tG.append(g)\n",
    "\t\t\tB.append(b)\n",
    "\n",
    "\t\t# add the image and label # to the HDF5 dataset\n",
    "\t\twriter.add([image], [label])\n",
    "\t\tpbar.update(i)\n",
    "\n",
    "\t# close the HDF5 writer\n",
    "\tpbar.finish()\n",
    "\twriter.close()\n",
    "\n",
    "# construct a dictionary of averages, then serialize the means to a\n",
    "# JSON file\n",
    "print(\"[INFO] serializing means...\")\n",
    "D = {\"R\": np.mean(R), \"G\": np.mean(G), \"B\": np.mean(B)}\n",
    "f = open(DATASET_MEAN, \"w\")\n",
    "f.write(json.dumps(D))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "HkXIZpCslRci",
    "outputId": "036c556c-e821-4d90-8f7b-6a7a5a3ba9ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0K\t./.config/configurations\n",
      "60K\t./.config/logs/2020.12.02\n",
      "64K\t./.config/logs\n",
      "100K\t./.config\n",
      "56M\t./data/hdf5\n",
      "31M\t./data/train\n",
      "4.0K\t./data/output\n",
      "87M\t./data\n",
      "55M\t./sample_data\n",
      "171M\t.\n"
     ]
    }
   ],
   "source": [
    "!du -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFt0U2oUmMuU"
   },
   "source": [
    "### Implementing AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ax0EAsPgmEea"
   },
   "outputs": [],
   "source": [
    "class AlexNet:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, reg=0.0002):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\" and the channels dimension itself\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# Block #1: first CONV => RELU => POOL layer set\n",
    "\t\tmodel.add(Conv2D(96, (11, 11), strides=(4, 4),input_shape=inputShape, padding=\"valid\",kernel_regularizer=l2(reg)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# Block #2: second CONV => RELU => POOL layer set\n",
    "\t\tmodel.add(Conv2D(256, (5, 5), padding=\"same\",kernel_regularizer=l2(reg)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# Block #3: CONV => RELU => CONV => RELU => CONV => RELU\n",
    "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(384, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(256, (3, 3), padding=\"same\",kernel_regularizer=l2(reg)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# Block #4: first set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\n",
    "\t\t# Block #5: second set of FC => RELU layers\n",
    "\t\tmodel.add(Dense(4096, kernel_regularizer=l2(reg)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tmodel.add(Dense(classes, kernel_regularizer=l2(reg)))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeWP4j7-nGuU"
   },
   "source": [
    "## 3.2 Training AlexNet on Kaggle: dogs vs cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FXM6nqiaqBh1"
   },
   "outputs": [],
   "source": [
    "# define the path to the output training, validation, and testing\n",
    "# HDF5 files\n",
    "TRAIN_HDF5 = \"data/hdf5/train.hdf5\"\n",
    "VAL_HDF5 = \"data/hdf5/val.hdf5\"\n",
    "TEST_HDF5 = \"data/hdf5/test.hdf5\"\n",
    "\n",
    "# path to the output model file\n",
    "MODEL_PATH = \"data/cocamar.model\"\n",
    "\n",
    "# define the path to the dataset mean\n",
    "DATASET_MEAN = \"data/cocamar.json\"\n",
    "\n",
    "# define the path to the output directory used for storing plots,\n",
    "# classification reports, etc.\n",
    "OUTPUT_PATH = \"cocamar/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "MmNyJTrMpfeY",
    "outputId": "1129d43a-1d08-4dac-def2-0eb0aa83839c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "Epoch 2/100\n",
      "97/97 [==============================] - 171s 2s/step - loss: 3.1882 - accuracy: 0.4971 - val_loss: 4.0719 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00002: val_accuracy improved from -inf to 0.51429, saving model to cocamar/epochs:002-val_acc:0.514.hdf5\n",
      "Epoch 3/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 2.0929 - accuracy: 0.9520 - val_loss: 2.3154 - val_accuracy: 0.4571\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.51429\n",
      "Epoch 4/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 1.5241 - accuracy: 0.9806 - val_loss: 1.7671 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.51429 to 0.65714, saving model to cocamar/epochs:004-val_acc:0.657.hdf5\n",
      "Epoch 5/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 1.1383 - accuracy: 0.9747 - val_loss: 1.8866 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.65714\n",
      "Epoch 6/100\n",
      "97/97 [==============================] - 159s 2s/step - loss: 0.8836 - accuracy: 0.9836 - val_loss: 1.6771 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.65714\n",
      "Epoch 7/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.7190 - accuracy: 0.9864 - val_loss: 1.2820 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.65714\n",
      "Epoch 8/100\n",
      "97/97 [==============================] - 159s 2s/step - loss: 0.5184 - accuracy: 0.9937 - val_loss: 1.2661 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.65714\n",
      "Epoch 9/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.5156 - accuracy: 0.9783 - val_loss: 0.9284 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.65714 to 0.74286, saving model to cocamar/epochs:009-val_acc:0.743.hdf5\n",
      "Epoch 10/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 0.3918 - accuracy: 0.9927 - val_loss: 1.3730 - val_accuracy: 0.6286\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.74286\n",
      "Epoch 11/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 0.3750 - accuracy: 0.9943 - val_loss: 0.8805 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.74286\n",
      "Epoch 12/100\n",
      "97/97 [==============================] - 162s 2s/step - loss: 0.3727 - accuracy: 0.9772 - val_loss: 1.3081 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.74286\n",
      "Epoch 13/100\n",
      "97/97 [==============================] - 164s 2s/step - loss: 0.3716 - accuracy: 0.9956 - val_loss: 0.9027 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.74286\n",
      "Epoch 14/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 0.2851 - accuracy: 0.9890 - val_loss: 1.0203 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.74286\n",
      "Epoch 15/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 0.2692 - accuracy: 0.9831 - val_loss: 1.3344 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.74286\n",
      "Epoch 16/100\n",
      "97/97 [==============================] - 162s 2s/step - loss: 0.2766 - accuracy: 0.9835 - val_loss: 1.1780 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.74286\n",
      "Epoch 17/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.1997 - accuracy: 0.9982 - val_loss: 0.9908 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.74286\n",
      "Epoch 18/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.1880 - accuracy: 0.9919 - val_loss: 1.5134 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.74286\n",
      "Epoch 19/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 0.3115 - accuracy: 0.9820 - val_loss: 0.9673 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.74286\n",
      "Epoch 20/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.2709 - accuracy: 0.9862 - val_loss: 0.8257 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.74286\n",
      "Epoch 21/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.1832 - accuracy: 0.9949 - val_loss: 0.8062 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.74286\n",
      "Epoch 22/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.2306 - accuracy: 0.9883 - val_loss: 1.3158 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.74286\n",
      "Epoch 23/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.2297 - accuracy: 0.9871 - val_loss: 1.2260 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.74286\n",
      "Epoch 24/100\n",
      "97/97 [==============================] - 158s 2s/step - loss: 0.2394 - accuracy: 0.9851 - val_loss: 1.2564 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.74286\n",
      "Epoch 25/100\n",
      "97/97 [==============================] - 165s 2s/step - loss: 0.2304 - accuracy: 0.9863 - val_loss: 1.7807 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.74286\n",
      "Epoch 26/100\n",
      "97/97 [==============================] - 169s 2s/step - loss: 0.2875 - accuracy: 0.9864 - val_loss: 0.9594 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.74286\n",
      "Epoch 27/100\n",
      "97/97 [==============================] - 167s 2s/step - loss: 0.3140 - accuracy: 0.9835 - val_loss: 1.0771 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.74286\n",
      "Epoch 28/100\n",
      "97/97 [==============================] - 169s 2s/step - loss: 0.2354 - accuracy: 0.9885 - val_loss: 0.9812 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.74286\n",
      "Epoch 29/100\n",
      "97/97 [==============================] - 168s 2s/step - loss: 0.1969 - accuracy: 0.9923 - val_loss: 2.0620 - val_accuracy: 0.4571\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.74286\n",
      "Epoch 30/100\n",
      "97/97 [==============================] - 167s 2s/step - loss: 0.2617 - accuracy: 0.9853 - val_loss: 1.6568 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.74286\n",
      "Epoch 31/100\n",
      "97/97 [==============================] - 169s 2s/step - loss: 0.2653 - accuracy: 0.9849 - val_loss: 1.1524 - val_accuracy: 0.6286\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.74286\n",
      "Epoch 32/100\n",
      "97/97 [==============================] - 167s 2s/step - loss: 0.3278 - accuracy: 0.9823 - val_loss: 1.3846 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.74286\n",
      "Epoch 33/100\n",
      "97/97 [==============================] - 167s 2s/step - loss: 0.2609 - accuracy: 0.9895 - val_loss: 1.2398 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.74286\n",
      "Epoch 34/100\n",
      "97/97 [==============================] - 164s 2s/step - loss: 0.2890 - accuracy: 0.9911 - val_loss: 1.1131 - val_accuracy: 0.4571\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.74286\n",
      "Epoch 35/100\n",
      "97/97 [==============================] - 167s 2s/step - loss: 0.2073 - accuracy: 0.9927 - val_loss: 1.7130 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.74286\n",
      "Epoch 36/100\n",
      "97/97 [==============================] - 173s 2s/step - loss: 0.2247 - accuracy: 0.9924 - val_loss: 1.3144 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.74286\n",
      "Epoch 37/100\n",
      "97/97 [==============================] - 174s 2s/step - loss: 0.2596 - accuracy: 0.9854 - val_loss: 1.7292 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.74286\n",
      "Epoch 38/100\n",
      "97/97 [==============================] - 173s 2s/step - loss: 0.2730 - accuracy: 0.9871 - val_loss: 1.8641 - val_accuracy: 0.6286\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.74286\n",
      "Epoch 39/100\n",
      "97/97 [==============================] - 175s 2s/step - loss: 0.3484 - accuracy: 0.9862 - val_loss: 1.1551 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.74286\n",
      "Epoch 40/100\n",
      "97/97 [==============================] - 172s 2s/step - loss: 0.2573 - accuracy: 0.9911 - val_loss: 1.1030 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.74286\n",
      "Epoch 41/100\n",
      "97/97 [==============================] - 174s 2s/step - loss: 0.2383 - accuracy: 0.9917 - val_loss: 1.5411 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.74286\n",
      "Epoch 42/100\n",
      "97/97 [==============================] - 174s 2s/step - loss: 0.2899 - accuracy: 0.9880 - val_loss: 1.6983 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.74286\n",
      "Epoch 43/100\n",
      "97/97 [==============================] - 172s 2s/step - loss: 0.2400 - accuracy: 0.9909 - val_loss: 1.4608 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.74286\n",
      "Epoch 44/100\n",
      "97/97 [==============================] - 170s 2s/step - loss: 0.2369 - accuracy: 0.9935 - val_loss: 1.1478 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.74286\n",
      "Epoch 45/100\n",
      "97/97 [==============================] - 171s 2s/step - loss: 0.3266 - accuracy: 0.9903 - val_loss: 0.9703 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.74286\n",
      "Epoch 46/100\n",
      "97/97 [==============================] - 171s 2s/step - loss: 0.2584 - accuracy: 0.9877 - val_loss: 1.3079 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.74286\n",
      "Epoch 47/100\n",
      "97/97 [==============================] - 173s 2s/step - loss: 0.3368 - accuracy: 0.9799 - val_loss: 1.5520 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.74286\n",
      "Epoch 48/100\n",
      "97/97 [==============================] - 172s 2s/step - loss: 0.2847 - accuracy: 0.9865 - val_loss: 0.9473 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.74286\n",
      "Epoch 49/100\n",
      "97/97 [==============================] - 168s 2s/step - loss: 0.2794 - accuracy: 0.9883 - val_loss: 1.0663 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.74286\n",
      "Epoch 50/100\n",
      "97/97 [==============================] - 165s 2s/step - loss: 0.2548 - accuracy: 0.9899 - val_loss: 1.9344 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.74286\n",
      "Epoch 51/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 0.3185 - accuracy: 0.9878 - val_loss: 1.3969 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.74286\n",
      "Epoch 52/100\n",
      "97/97 [==============================] - 162s 2s/step - loss: 0.2337 - accuracy: 0.9920 - val_loss: 1.3541 - val_accuracy: 0.4571\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.74286\n",
      "Epoch 53/100\n",
      "97/97 [==============================] - 166s 2s/step - loss: 0.2154 - accuracy: 0.9917 - val_loss: 1.6901 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.74286\n",
      "Epoch 54/100\n",
      "97/97 [==============================] - 169s 2s/step - loss: 0.2612 - accuracy: 0.9926 - val_loss: 1.1512 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.74286\n",
      "Epoch 55/100\n",
      "97/97 [==============================] - 169s 2s/step - loss: 0.2785 - accuracy: 0.9903 - val_loss: 1.5200 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.74286\n",
      "Epoch 56/100\n",
      "97/97 [==============================] - 166s 2s/step - loss: 0.2643 - accuracy: 0.9886 - val_loss: 1.0828 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.74286\n",
      "Epoch 57/100\n",
      "97/97 [==============================] - 168s 2s/step - loss: 0.3440 - accuracy: 0.9807 - val_loss: 1.5756 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.74286\n",
      "Epoch 58/100\n",
      "97/97 [==============================] - 168s 2s/step - loss: 0.3174 - accuracy: 0.9879 - val_loss: 1.1717 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.74286\n",
      "Epoch 59/100\n",
      "97/97 [==============================] - 168s 2s/step - loss: 0.3290 - accuracy: 0.9859 - val_loss: 1.3885 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.74286\n",
      "Epoch 60/100\n",
      "97/97 [==============================] - 168s 2s/step - loss: 0.2756 - accuracy: 0.9915 - val_loss: 1.6318 - val_accuracy: 0.4571\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.74286\n",
      "Epoch 61/100\n",
      "97/97 [==============================] - 169s 2s/step - loss: 0.2896 - accuracy: 0.9896 - val_loss: 1.1635 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.74286\n",
      "Epoch 62/100\n",
      "97/97 [==============================] - 167s 2s/step - loss: 0.2650 - accuracy: 0.9918 - val_loss: 0.7819 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.74286\n",
      "Epoch 63/100\n",
      "97/97 [==============================] - 166s 2s/step - loss: 0.1948 - accuracy: 0.9924 - val_loss: 1.2869 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.74286\n",
      "Epoch 64/100\n",
      "97/97 [==============================] - 166s 2s/step - loss: 0.2594 - accuracy: 0.9884 - val_loss: 1.2486 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.74286\n",
      "Epoch 65/100\n",
      "97/97 [==============================] - 168s 2s/step - loss: 0.3021 - accuracy: 0.9884 - val_loss: 1.1626 - val_accuracy: 0.4571\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.74286\n",
      "Epoch 66/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 0.2120 - accuracy: 0.9930 - val_loss: 6.0326 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.74286\n",
      "Epoch 67/100\n",
      "97/97 [==============================] - 164s 2s/step - loss: 0.2904 - accuracy: 0.9868 - val_loss: 0.9951 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.74286\n",
      "Epoch 68/100\n",
      "97/97 [==============================] - 164s 2s/step - loss: 0.2395 - accuracy: 0.9925 - val_loss: 1.2402 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.74286\n",
      "Epoch 69/100\n",
      "97/97 [==============================] - 164s 2s/step - loss: 0.2392 - accuracy: 0.9893 - val_loss: 1.3838 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.74286\n",
      "Epoch 70/100\n",
      "97/97 [==============================] - 162s 2s/step - loss: 0.2696 - accuracy: 0.9883 - val_loss: 1.0805 - val_accuracy: 0.6286\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.74286\n",
      "Epoch 71/100\n",
      "97/97 [==============================] - 162s 2s/step - loss: 0.2709 - accuracy: 0.9880 - val_loss: 1.3003 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.74286\n",
      "Epoch 72/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.2492 - accuracy: 0.9884 - val_loss: 1.0986 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.74286\n",
      "Epoch 73/100\n",
      "97/97 [==============================] - 162s 2s/step - loss: 0.2225 - accuracy: 0.9941 - val_loss: 1.5599 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.74286\n",
      "Epoch 74/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.2404 - accuracy: 0.9910 - val_loss: 1.0317 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.74286\n",
      "Epoch 75/100\n",
      "97/97 [==============================] - 169s 2s/step - loss: 0.2407 - accuracy: 0.9913 - val_loss: 0.9846 - val_accuracy: 0.6286\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.74286\n",
      "Epoch 76/100\n",
      "97/97 [==============================] - 170s 2s/step - loss: 0.2383 - accuracy: 0.9874 - val_loss: 1.2769 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.74286\n",
      "Epoch 77/100\n",
      "97/97 [==============================] - 165s 2s/step - loss: 0.2863 - accuracy: 0.9855 - val_loss: 1.5330 - val_accuracy: 0.4286\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.74286\n",
      "Epoch 78/100\n",
      "97/97 [==============================] - 162s 2s/step - loss: 0.2301 - accuracy: 0.9931 - val_loss: 1.1248 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.74286\n",
      "Epoch 79/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.2368 - accuracy: 0.9895 - val_loss: 1.5615 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.74286\n",
      "Epoch 80/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.1892 - accuracy: 0.9944 - val_loss: 1.8052 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.74286\n",
      "Epoch 81/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.2102 - accuracy: 0.9928 - val_loss: 1.8029 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.74286\n",
      "Epoch 82/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.2232 - accuracy: 0.9917 - val_loss: 1.0243 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.74286\n",
      "Epoch 83/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.1915 - accuracy: 0.9921 - val_loss: 0.8797 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.74286\n",
      "Epoch 84/100\n",
      "97/97 [==============================] - 164s 2s/step - loss: 0.1908 - accuracy: 0.9958 - val_loss: 1.4455 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.74286\n",
      "Epoch 85/100\n",
      "97/97 [==============================] - 165s 2s/step - loss: 0.2098 - accuracy: 0.9898 - val_loss: 1.1010 - val_accuracy: 0.6571\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.74286\n",
      "Epoch 86/100\n",
      "97/97 [==============================] - 165s 2s/step - loss: 0.2490 - accuracy: 0.9929 - val_loss: 1.1214 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.74286\n",
      "Epoch 87/100\n",
      "97/97 [==============================] - 171s 2s/step - loss: 0.2419 - accuracy: 0.9884 - val_loss: 0.9133 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.74286\n",
      "Epoch 88/100\n",
      "97/97 [==============================] - 169s 2s/step - loss: 0.2080 - accuracy: 0.9923 - val_loss: 1.0673 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.74286\n",
      "Epoch 89/100\n",
      "97/97 [==============================] - 163s 2s/step - loss: 0.1964 - accuracy: 0.9927 - val_loss: 0.8974 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.74286\n",
      "Epoch 90/100\n",
      "97/97 [==============================] - 162s 2s/step - loss: 0.2057 - accuracy: 0.9922 - val_loss: 1.1749 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.74286\n",
      "Epoch 91/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.1970 - accuracy: 0.9954 - val_loss: 1.4760 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.74286\n",
      "Epoch 92/100\n",
      "97/97 [==============================] - 159s 2s/step - loss: 0.2879 - accuracy: 0.9803 - val_loss: 0.7692 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.74286\n",
      "Epoch 93/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.1755 - accuracy: 0.9955 - val_loss: 1.0684 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.74286\n",
      "Epoch 94/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.2181 - accuracy: 0.9912 - val_loss: 1.4504 - val_accuracy: 0.4571\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.74286\n",
      "Epoch 95/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.2366 - accuracy: 0.9901 - val_loss: 0.9397 - val_accuracy: 0.6286\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.74286\n",
      "Epoch 96/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.1686 - accuracy: 0.9954 - val_loss: 0.7239 - val_accuracy: 0.6857\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.74286\n",
      "Epoch 97/100\n",
      "97/97 [==============================] - 161s 2s/step - loss: 0.1919 - accuracy: 0.9938 - val_loss: 1.2247 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.74286\n",
      "Epoch 98/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.1711 - accuracy: 0.9924 - val_loss: 1.2198 - val_accuracy: 0.4857\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.74286\n",
      "Epoch 99/100\n",
      "97/97 [==============================] - 160s 2s/step - loss: 0.2225 - accuracy: 0.9922 - val_loss: 1.3409 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.74286\n",
      "Epoch 100/100\n",
      "97/97 [==============================] - 159s 2s/step - loss: 0.1921 - accuracy: 0.9932 - val_loss: 1.0336 - val_accuracy: 0.5429\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.74286\n",
      "[INFO] serializing model...\n",
      "INFO:tensorflow:Assets written to: data/cocamar.model/assets\n"
     ]
    }
   ],
   "source": [
    "resume = False\n",
    "\n",
    "# checkpoint files\n",
    "filepath= \"cocamar/epochs:{epoch:03d}-val_acc:{val_accuracy:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
    "                         width_shift_range=0.2,\n",
    "                         height_shift_range=0.2,\n",
    "                         shear_range=0.15,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# load the RGB means for the training set\n",
    "means = json.loads(open(DATASET_MEAN).read())\n",
    "\n",
    "# initialize the image preprocessors\n",
    "sp = SimplePreprocessor(227, 227)\n",
    "pp = PatchPreprocessor(227, 227)\n",
    "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
    "iap = ImageToArrayPreprocessor()\n",
    "\n",
    "# initialize the training and validation dataset generators\n",
    "trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug,preprocessors=[pp, mp, iap], classes=5)\n",
    "valGen = HDF5DatasetGenerator(VAL_HDF5, 128,preprocessors=[sp, mp, iap], classes=5)\n",
    "\n",
    "# initialize the optimizer\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=1e-3)\n",
    "\n",
    "model = AlexNet.build(width=227, height=227, depth=3,classes=5, reg=0.0002)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "# construct the set of callbacks\n",
    "path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\n",
    "callbacks = [TrainingMonitor(path)]\n",
    "\n",
    "initial_epoch = 1\n",
    "\n",
    "# load previou weights\n",
    "if resume == True:\n",
    "  model.load_weights(\"cocamar/epochs:009-val_acc:0.743.hdf5\")\n",
    "  initial_epoch = 45\n",
    "\n",
    "# train the network\n",
    "history = model.fit(trainGen.generator(),\n",
    "          steps_per_epoch=trainGen.numImages // 2,\n",
    "          validation_data=valGen.generator(),\n",
    "          validation_steps=valGen.numImages // 2,\n",
    "          epochs=100,\n",
    "          max_queue_size=10,\n",
    "          callbacks=[callbacks,checkpoint], verbose=1,\n",
    "          initial_epoch=initial_epoch)\n",
    "\n",
    "# save the model to file\n",
    "print(\"[INFO] serializing model...\")\n",
    "model.save(MODEL_PATH, overwrite=True)\n",
    "\n",
    "# close the HDF5 datasets\n",
    "trainGen.close()\n",
    "valGen.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgP_SX7JBvSK"
   },
   "source": [
    "### Evaluating AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "A8nruX7p86lq",
    "outputId": "90980b22-96a4-46e7-d26f-603babd5bd7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] predicting on test data (no crops)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% |#                                            | ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] rank-1: 15.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100% |#                                            | ETA:  --:--:--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] predicting on test data (with crops)...\n",
      "[INFO] rank-1: 68.57%\n"
     ]
    }
   ],
   "source": [
    "means = json.loads(open(DATASET_MEAN).read())\n",
    "\n",
    "# initialize the image preprocessors\n",
    "sp = SimplePreprocessor(227, 227)\n",
    "mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n",
    "cp = CropPreprocessor(227, 227)\n",
    "iap = ImageToArrayPreprocessor()\n",
    "\n",
    "# load the pretrained network\n",
    "print(\"[INFO] loading model...\")\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "# initialize the testing dataset generator, then make predictions on\n",
    "# the testing data\n",
    "print(\"[INFO] predicting on test data (no crops)...\")\n",
    "testGen = HDF5DatasetGenerator(TEST_HDF5, 64,\n",
    "                               preprocessors=[sp, mp, iap], classes=5)\n",
    "predictions = model.predict(testGen.generator(),\n",
    "                            steps=testGen.numImages // 8, max_queue_size=10)\n",
    "\n",
    "# compute the rank-1 and rank-5 accuracies\n",
    "(rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n",
    "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n",
    "testGen.close()\n",
    "\n",
    "\n",
    "# re-initialize the testing set generator, this time excluding the\n",
    "# `SimplePreprocessor`\n",
    "testGen = HDF5DatasetGenerator(TEST_HDF5, 64,\n",
    "                               preprocessors=[mp], classes=5)\n",
    "predictions = []\n",
    "\n",
    "# initialize the progress bar\n",
    "widgets = [\"Evaluating: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "pbar = progressbar.ProgressBar(maxval=testGen.numImages // 64,widgets=widgets).start()\n",
    "\n",
    "# loop over a single pass of the test data\n",
    "for (i, (images, labels)) in enumerate(testGen.generator(passes=1)):\n",
    "\t# loop over each of the individual images\n",
    "\tfor image in images:\n",
    "\t\t# apply the crop preprocessor to the image to generate 10\n",
    "\t\t# separate crops, then convert them from images to arrays\n",
    "\t\tcrops = cp.preprocess(image)\n",
    "\t\tcrops = np.array([iap.preprocess(c) for c in crops],\n",
    "\t\t\tdtype=\"float32\")\n",
    "\n",
    "\t\t# make predictions on the crops and then average them\n",
    "\t\t# together to obtain the final prediction\n",
    "\t\tpred = model.predict(crops)\n",
    "\t\tpredictions.append(pred.mean(axis=0))\n",
    "\n",
    "\t# update the progress bar\n",
    "\tpbar.update(i)\n",
    "\n",
    "# compute the rank-1 accuracy\n",
    "pbar.finish()\n",
    "print(\"[INFO] predicting on test data (with crops)...\")\n",
    "(rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n",
    "print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n",
    "testGen.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSkfaOgfMd1x",
    "outputId": "b2e2edcf-094d-4665-ce97-e698a6d9ebbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database keys ['features', 'label_names', 'labels']\n",
      "[INFO] tuning hyperparameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] best hyperparameters: {'C': 0.0001}\n",
      "[INFO] evaluating...\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          lagarta       1.00      0.88      0.93         8\n",
      "         negative       0.80      1.00      0.89        32\n",
      " percevejo_marrom       1.00      0.76      0.87        17\n",
      "percevejo_pequeno       1.00      1.00      1.00         2\n",
      "  percevejo_verde       0.80      0.50      0.62         8\n",
      "\n",
      "         accuracy                           0.87        67\n",
      "        macro avg       0.92      0.83      0.86        67\n",
      "     weighted avg       0.88      0.87      0.86        67\n",
      "\n",
      "[INFO] saving model...\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VrtfzCJ-553h"
   },
   "outputs": [],
   "source": [
    "!cp -r cocamar/ /content/drive/MyDrive/Projeto/\n",
    "!cp -r data/ /content/drive/MyDrive/Projeto/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0j_yum-BGxn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8boDWfTt6a97",
    "DcfAFunl6jY0"
   ],
   "include_colab_link": true,
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
